{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81000,"databundleVersionId":8812083,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook we test a set of simple neural networks to predict crop yield from climate data.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-26T12:36:07.110575Z","iopub.execute_input":"2025-07-26T12:36:07.111292Z","iopub.status.idle":"2025-07-26T12:36:07.122361Z","shell.execute_reply.started":"2025-07-26T12:36:07.111269Z","shell.execute_reply":"2025-07-26T12:36:07.121388Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/the-future-crop-challenge/pr_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/sample_submission.csv\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_maize.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/pr_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_wheat.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_train.parquet\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Simple RNN\n\nA good place to start is to train a very simple RNN on the data.\n\nWithout fancy tricks, we try to build a latent predictor which generalises across locations on the globe.\n\nThe RNN here is trained to predict future climate data inputs, as well as the final yield at each timepoint.\nIdeally this let's the RNN give a ballpark guess from the beginning which is iteratively updated.\n\nCredit assignment is thereby be given at each timepoint relative to the (final) target yield.","metadata":{}},{"cell_type":"code","source":"# Setup #\nimport os\nimport copy\nimport random\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import StandardScaler\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDATA_DIR = r'/kaggle/input/the-future-crop-challenge'\nprint(f'Running on {device}')\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    return\n\nsetup_seed(2025) #set seed to current year","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T12:36:10.443789Z","iopub.execute_input":"2025-07-26T12:36:10.444561Z","iopub.status.idle":"2025-07-26T12:36:15.485266Z","shell.execute_reply.started":"2025-07-26T12:36:10.444522Z","shell.execute_reply":"2025-07-26T12:36:15.484424Z"}},"outputs":[{"name":"stdout","text":"Running on cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"## Set up dataset with pytorch ##\nclass ClimateDataset(Dataset):\n    \"\"\"\n    The ClimateDataset class provides a convenient way for acessing, merging and scaling parquet data. \n    This is used in the main training loop to access features and target variables.\n    \"\"\"\n    def __init__(self, crop: str, mode: str, data_dir: str, scalers: list = None):\n        self.tasmax = pd.read_parquet(os.path.join(data_dir, f\"tasmax_{crop}_{mode}.parquet\"))\n        self.tasmin = pd.read_parquet(os.path.join(data_dir, f\"tasmin_{crop}_{mode}.parquet\"))\n        # self.tas = pd.read_parquet(os.path.join(data_dir, f\"tas_{crop}_{mode}.parquet\"))\n        self.pr = pd.read_parquet(os.path.join(data_dir, f\"pr_{crop}_{mode}.parquet\"))\n        self.rsds = pd.read_parquet(os.path.join(data_dir, f\"rsds_{crop}_{mode}.parquet\"))\n        self.soil_co2 = pd.read_parquet(os.path.join(data_dir, f\"soil_co2_{crop}_{mode}.parquet\"))\n        \n        if mode == 'train':\n            self.yield_ = pd.read_parquet(os.path.join(data_dir, f\"{mode}_solutions_{crop}.parquet\"))\n        else:\n            self.yield_ = None\n\n        if scalers is None:\n            self._init_scalers()\n        else:\n            self.scaler_climate, self.scaler_soil, self.scaler_yield = scalers\n        self._check_data([self.tasmax, self.tasmin, self.pr, self.rsds, self.soil_co2], self.yield_)\n\n    def __getitem__(self, index):\n        # 240x4 climate matrix per location/year (features in last dimension by convention) + cumulative rsds\n        # first 5 indices are categorical, first 30 entries are before seeding, last 210 are after.\n        climate = np.vstack([\n            self.tasmax.iloc[index, 35:].astype(np.float32), \n            self.tasmin.iloc[index, 35:].astype(np.float32),\n            # self.tas.iloc[index, 35:].astype(np.float32),\n            self.pr.iloc[index, 35:].astype(np.float32),\n            self.rsds.iloc[index, 35:].astype(np.float32),\n            #np.cumsum(self.rsds.iloc[index, 35:]).astype(np.float32),\n        ]).T\n\n        # Fixed soil properties per location/year\n        soil = self.soil_co2.iloc[index][['lon','lat','co2', 'nitrogen']].astype(np.float32)\n        texture = self.soil_co2.iloc[index][['texture_class']].values.astype(np.int64)\n        id = soil.name\n        soil = soil.values\n\n        # Yield estimated by process model\n        if self.yield_ is not None:\n            yield_ = self.yield_.iloc[index].astype(np.float32).values\n        else:\n            yield_ = []\n\n        #climate_scaler = self.scaler_climate.transform(climate)\n        soil = self.scaler_soil.transform(soil.reshape(1, -1)).reshape(-1)\n        texture = F.one_hot(torch.from_numpy((texture-1).astype(np.int64)),13)[0].to(torch.float32)\n        soil_texture = torch.concat([torch.tensor(soil),texture])\n        # print(yield_)\n        return torch.tensor(climate), soil_texture, torch.tensor(yield_), id\n\n    def __len__(self):\n        return self.tasmax.shape[0]\n\n    def _init_scalers(self):\n        # Draw random sample from climate data to estimate distribution moments for scaler.\n        climate_sample = np.vstack([\n            self.tasmax.sample(1000).iloc[:, 5:].values.flatten(), \n            self.tasmin.sample(1000).iloc[:, 5:].values.flatten(),\n            # self.tas.sample(1000).iloc[:, 5:].values.flatten(),\n            self.pr.sample(1000).iloc[:, 5:].values.flatten(),\n            self.rsds.sample(1000).iloc[:, 5:].values.flatten(),\n            np.cumsum(self.rsds.sample(1000).iloc[:, 5:],1).values.flatten(),\n        ]).T\n        self.scaler_climate = StandardScaler()\n        self.scaler_climate.fit(climate_sample)\n        # Scaler for fixed soil properties\n        self.scaler_soil = StandardScaler()\n        self.scaler_soil.fit(self.soil_co2[['lon','lat','co2', 'nitrogen']].values)\n\n        # Scaler for yield\n        self.scaler_yield = StandardScaler()\n        if self.yield_ is not None:\n            self.scaler_yield.fit(self.yield_.values)\n\n    def _check_data(self, climate: list, yield_: pd.DataFrame) -> bool:\n        # Check for matching year, lon, lat columns\n        for i in range(1, len(climate)):\n            assert np.all(climate[0][['year', 'lon', 'lat']] == climate[i][['year', 'lon', 'lat']])\n        # Check label for matching length\n        assert yield_ is None or climate[0].shape[0] == yield_.shape[0]\n\n\n\nclass PredDataset(Dataset):\n    \"\"\"\n    The ClimateDataset class provides a convenient way for acessing, merging and scaling parquet data. \n    This is used in the main training loop to access features and target variables.\n    \"\"\"\n    def __init__(self, crop: str, mode: str, data_dir: str, device = device):\n\n        self.climate_data = {}\n        for key in ['tasmax','tasmin','pr','rsds','soil_co2']: #data to be loaded onto GPU\n            data_df_cpu = pd.read_parquet(os.path.join(data_dir, f\"{key}_{crop}_{mode}.parquet\"))\n            if not key == 'soil_co2':\n                # first 5 indices are categorical, next 30 entries are before seeding, last 210 are after.\n                self.climate_data[key] = torch.tensor(data_df_cpu.iloc[:,5:].values).to(dtype=torch.float32,device=device)\n            elif key == 'soil_co2':\n                soil = data_df_cpu.iloc[:][['lon','lat','co2', 'nitrogen']].astype(np.float32)\n                texture = data_df_cpu.iloc[:][['texture_class']].values.astype(np.int64)\n                texture = F.one_hot(torch.from_numpy((texture-1)),13).squeeze().to(dtype=torch.float32, device=device)\n                soil_texture = torch.concat([torch.tensor(soil.values).to(device = device),texture],axis=-1)\n                print(soil_texture.shape)\n                self.climate_data[key] = soil_texture.unsqueeze(1).expand(soil_texture.shape[0],240,soil_texture.shape[-1])\n                self.climate_data['id'] = soil.index \n                \n        if mode == 'train':\n            self.yield_ = pd.read_parquet(os.path.join(data_dir, f\"{mode}_solutions_{crop}.parquet\"))\n        else:\n            self.yield_ = None\n\n        #self._check_data([v for k,v in self.climate_data.items], self.yield_)\n\n    def __getitem__(self, index):\n        # 240x4 climate matrix per location/year (features in last dimension by convention) + cumulative rsds\n        climate_inputs = torch.concat([v[index].unsqueeze(-1) for k,v in self.climate_data.items() if k not in ['id','soil_co2']],axis=-1)\n        climate_inputs = torch.concat([climate_inputs,self.climate_data['soil_co2'][index]],axis=-1)\n\n        # Yield estimated by process model\n        if self.yield_ is not None:\n            yield_ = self.yield_.iloc[index].astype(np.float32).values\n        else:\n            yield_ = []\n\n        return torch.tensor(climate_inputs, device= device), torch.tensor(yield_, device = device)\n\n    def __len__(self):\n        return self.climate_data['tasmax'].shape[0]\n\n    def _check_data(self, climate: list, yield_: pd.DataFrame) -> bool:\n        # Check for matching year, lon, lat columns\n        for i in range(1, len(climate)):\n            assert np.all(climate[0][['year', 'lon', 'lat']] == climate[i][['year', 'lon', 'lat']])\n        # Check label for matching length\n        assert yield_ is None or climate[0].shape[0] == yield_.shape[0]\n\n\n#test generating the data\nds_train = PredDataset(crop = 'wheat',mode='train', data_dir = DATA_DIR)\nprint('succesful dataset generation')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T12:30:53.880204Z","iopub.execute_input":"2025-07-26T12:30:53.880909Z","iopub.status.idle":"2025-07-26T12:31:00.368650Z","shell.execute_reply.started":"2025-07-26T12:30:53.880884Z","shell.execute_reply":"2025-07-26T12:31:00.367900Z"}},"outputs":[{"name":"stdout","text":"torch.Size([278747, 17])\nsuccesful dataset generation\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# define model #\n\nclass PredictiveRNN(nn.Module):\n    def __init__(self, n_inputs, n_hidden, n_targets, n_layers, nonlinearity = 'tanh'):\n        super(PredictiveRNN, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_inputs = n_inputs\n        self.n_targets = n_targets\n\n        self.rnn = nn.RNN(n_inputs, n_hidden, n_layers, nonlinearity=nonlinearity, \n                          batch_first = True)\n        self.encoder = nn.Linear(n_inputs, n_hidden)\n        self.decoder = nn.Linear(n_hidden, n_targets)\n        self.mse = nn.MSELoss()\n\n        #initialise weights for improved gradient descent\n        for name, param in self.named_parameters():\n            if 'weight' in name:\n                if nonlinearity == 'tanh':\n                    nn.init.xavier_uniform_(param)\n                elif nonlinearity == 'relu':\n                    nn.init.kaiming_uniform_(param)           \n        \n    def forward(self, inputs):\n        #we want to initialise the hidden state with encoder weights rather than 0s\n        #these are esentially context-setting weights.\n        h0 = self.encoder(inputs[:,0,:])\n        h0 = h0.unsqueeze(0) #(1,batch_size,1)\n        rnn_out, hidden = self.rnn(inputs, h0)\n        predictions = self.decoder(rnn_out)\n\n        return predictions\n\n    def compute_losses(self, predictions, targets):\n        pred_loss = self.mse(predictions[:,:,:5], targets[:,:,:5])\n        yield_loss = self.mse(predictions[:,-1,-1], targets[:,-1,-1])\n        return pred_loss, yield_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T12:33:51.176688Z","iopub.execute_input":"2025-07-26T12:33:51.176986Z","iopub.status.idle":"2025-07-26T12:33:51.183645Z","shell.execute_reply.started":"2025-07-26T12:33:51.176964Z","shell.execute_reply":"2025-07-26T12:33:51.183119Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# Training loop #\n\n# global variables / options #\nBATCH_SIZE = 200\nN_EPOCHS = 1\nLEARNING_RATE = 3e-4\nN_HIDDEN = 256\nN_LAYERS = 1\nN_INPUT = 21\nN_TARGET = 6\n\n# set up model and optimizer # \nmodel = PredictiveRNN(N_INPUT, N_HIDDEN, N_TARGET,N_LAYERS,\n                     nonlinearity = 'tanh').to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n\n# prepare data loader and start training loop #\n\ntrain_loader = DataLoader(ds_train, batch_size= BATCH_SIZE, shuffle=True)\ndef train_model(train_loader, model, optimizer):\n    yield_losses = []\n    prediction_losses = []\n    for epoch in range(N_EPOCHS):\n        for i, (inputs, targets) in tqdm(enumerate(train_loader)):\n\n            torch.cuda.synchronize()\n            optimizer.zero_grad()\n            #run data through model and backprop losses\n            predictions = model(inputs)\n            pred_loss, yield_loss = model.compute_losses(predictions, targets)\n            loss = pred_loss+yield_loss\n            loss.backward()\n            optimizer.step()\n            torch.cuda.synchronize()\n            #append losses\n            yield_losses.append(yield_loss.detach().cpu().item())\n            prediction_losses.append(pred_loss.detach().cpu().item())\n    \n        print(f'Prediction loss: {round(prediction_losses[-1],3)}, yield loss: {round(yield_losses[-1],3)}')\n        \n    return model, yield_losses, prediction_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T12:34:05.947999Z","iopub.execute_input":"2025-07-26T12:34:05.948556Z","iopub.status.idle":"2025-07-26T12:34:10.203397Z","shell.execute_reply.started":"2025-07-26T12:34:05.948530Z","shell.execute_reply":"2025-07-26T12:34:10.202833Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"%load_ext line_profiler\n\ndef profile_dataloader():\n    for i, (climate, soil, true_yield, _) in tqdm(enumerate(train_loader)):\n        None\n    return None\n\n%lprun -f train_loader [None for climate, soil, true_yield, id in train_loader]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:16:39.240099Z","iopub.execute_input":"2025-07-20T16:16:39.240377Z","iopub.status.idle":"2025-07-20T16:16:42.576975Z","shell.execute_reply.started":"2025-07-20T16:16:39.240358Z","shell.execute_reply":"2025-07-20T16:16:42.576173Z"}},"outputs":[{"name":"stdout","text":"The line_profiler extension is already loaded. To reload it, use:\n  %reload_ext line_profiler\n*** KeyboardInterrupt exception caught in code being profiled.","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Timer unit: 1e-09 s"},"metadata":{}}],"execution_count":29}]}