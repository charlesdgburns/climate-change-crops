{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81000,"databundleVersionId":8812083,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nAfter looking at the FutureCrop data, it appears that there's a large variance across locations in the mean yield and also the variance.\n\nHere we will try to model each location independently- we then will try to scale this up to train the many models in parallel on a GPU using pytorch.\n\nThe idea is to learn the minimal mapping from time-series data to a prediction of the yield.\n\n**Notes**:\nA simple model just learns to predict the mean across all training batches of a single location. This is true for tiny (1-unit), shallow (100-unit wide) and deep networks (2-4 layers).\n\nWe need a smoother loss than MSE and some regularisation to improve on this.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata_dir = '/kaggle/input/the-future-crop-challenge/'\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:37:43.059057Z","iopub.execute_input":"2025-11-24T12:37:43.059397Z","iopub.status.idle":"2025-11-24T12:37:43.091856Z","shell.execute_reply.started":"2025-11-24T12:37:43.059370Z","shell.execute_reply":"2025-11-24T12:37:43.090588Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/the-future-crop-challenge/pr_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/sample_submission.csv\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_maize.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/pr_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_wheat.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_train.parquet\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"#climate data (timeseries)\n\n#soil_co2 and yields\n\nwheat_sco2 = pd.read_parquet('/kaggle/input/the-future-crop-challenge/soil_co2_wheat_train.parquet')\nwheat_yield = pd.read_parquet('/kaggle/input/the-future-crop-challenge/train_solutions_wheat.parquet')\nwheat_df = wheat_sco2.join(wheat_yield)\n\nmaize_sco2 = pd.read_parquet('/kaggle/input/the-future-crop-challenge/soil_co2_maize_train.parquet')\nmaize_yield = pd.read_parquet('/kaggle/input/the-future-crop-challenge/train_solutions_maize.parquet')\nmaize_df = maize_sco2.join(maize_yield)\n\nmean_df = pd.DataFrame()\nfor crop_df in [wheat_df,maize_df]:\n    temp_df = crop_df.groupby(['crop','lon','lat'], as_index = False).agg({'yield':['mean','std']})\n    temp_df.columns = ['crop','lon','lat','yield_mean','yield_std']\n    mean_df = pd.concat([mean_df,temp_df])\n\nmean_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:37:46.005306Z","iopub.execute_input":"2025-11-24T12:37:46.005663Z","iopub.status.idle":"2025-11-24T12:37:46.343341Z","shell.execute_reply.started":"2025-11-24T12:37:46.005638Z","shell.execute_reply":"2025-11-24T12:37:46.342391Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"       crop     lon    lat  yield_mean  yield_std\n0     wheat -123.25  44.75    4.965216   0.488492\n1     wheat -123.25  45.25    4.985947   0.501791\n2     wheat -123.25  45.75    4.822316   0.423168\n3     wheat -122.75  44.75    4.875486   0.537611\n4     wheat -122.75  45.25    5.379421   0.615185\n...     ...     ...    ...         ...        ...\n9298  maize  132.75  46.75    5.738692   1.191923\n9299  maize  132.75  47.25    8.622872   1.518404\n9300  maize  133.25  45.25    2.470256   0.359327\n9301  maize  133.25  47.25    6.932128   1.339325\n9302  maize  137.75  36.75    2.396718   0.298963\n\n[17966 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>crop</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>yield_mean</th>\n      <th>yield_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wheat</td>\n      <td>-123.25</td>\n      <td>44.75</td>\n      <td>4.965216</td>\n      <td>0.488492</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wheat</td>\n      <td>-123.25</td>\n      <td>45.25</td>\n      <td>4.985947</td>\n      <td>0.501791</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wheat</td>\n      <td>-123.25</td>\n      <td>45.75</td>\n      <td>4.822316</td>\n      <td>0.423168</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wheat</td>\n      <td>-122.75</td>\n      <td>44.75</td>\n      <td>4.875486</td>\n      <td>0.537611</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wheat</td>\n      <td>-122.75</td>\n      <td>45.25</td>\n      <td>5.379421</td>\n      <td>0.615185</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9298</th>\n      <td>maize</td>\n      <td>132.75</td>\n      <td>46.75</td>\n      <td>5.738692</td>\n      <td>1.191923</td>\n    </tr>\n    <tr>\n      <th>9299</th>\n      <td>maize</td>\n      <td>132.75</td>\n      <td>47.25</td>\n      <td>8.622872</td>\n      <td>1.518404</td>\n    </tr>\n    <tr>\n      <th>9300</th>\n      <td>maize</td>\n      <td>133.25</td>\n      <td>45.25</td>\n      <td>2.470256</td>\n      <td>0.359327</td>\n    </tr>\n    <tr>\n      <th>9301</th>\n      <td>maize</td>\n      <td>133.25</td>\n      <td>47.25</td>\n      <td>6.932128</td>\n      <td>1.339325</td>\n    </tr>\n    <tr>\n      <th>9302</th>\n      <td>maize</td>\n      <td>137.75</td>\n      <td>36.75</td>\n      <td>2.396718</td>\n      <td>0.298963</td>\n    </tr>\n  </tbody>\n</table>\n<p>17966 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"idx_lon = 132.75\nidx_lat = 47.25\n\nstatic_data = maize_df.query(f'lon=={idx_lon} and lat=={idx_lat}')[['co2','nitrogen','yield']]\n\ncrop = 'maize'\nmode = 'train'\n\ntasmax = pd.read_parquet(os.path.join(data_dir, f\"tasmax_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\ntasmin = pd.read_parquet(os.path.join(data_dir, f\"tasmin_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\npr = pd.read_parquet(os.path.join(data_dir, f\"pr_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\nrsds = pd.read_parquet(os.path.join(data_dir, f\"rsds_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\n\nclimate_data = np.stack([\n        tasmax.iloc[:, 5:].values,\n        tasmin.iloc[:, 5:].values,\n        pr.iloc[:, 5:].values,\n        rsds.iloc[:, 5:].values\n    ], axis=2)\n\nstatic_expanded = np.repeat(static_data.values[:,np.newaxis,:],240,axis=1)\n\nsequenced_data = np.concatenate([climate_data,static_expanded],axis = 2)\n\nsequenced_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:37:48.927728Z","iopub.execute_input":"2025-11-24T12:37:48.928018Z","iopub.status.idle":"2025-11-24T12:37:54.457879Z","shell.execute_reply.started":"2025-11-24T12:37:48.927999Z","shell.execute_reply":"2025-11-24T12:37:54.456817Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"(39, 240, 7)"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- Data Splitting ---\ninputs = sequenced_data[:, :, :-1]\ntargets = sequenced_data[:, -1, -1]\n\n# --- 1. Configuration ---\nn_batch = sequenced_data.shape[0] # Using full dataset as one batch\nn_seq = sequenced_data.shape[1]\nn_features = sequenced_data.shape[2] - 1\nn_hidden = 4\nn_layers = 1\nlearning_rate = 1e-1\nn_epochs = 1000\n\n# Regularization Hyperparameter (Adjust if needed)\nHIDDEN_REG_ALPHA = 1e-4 \n\n# Device configuration (Move all data/model to the device)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"--- Configuration Summary ---\")\nprint(f\"Data Shape (Batch, Seq, Feat): ({n_batch}, {n_seq}, {n_features})\")\nprint(f\"Hidden Size/Layers: {n_hidden}/{n_layers}\")\nprint(f\"Learning Rate/Epochs: {learning_rate}/{n_epochs}\")\nprint(f\"Using Device: {device}\")\nprint(f\"-----------------------------\")\n\n\n# --- 2. PyTorch Dataset & DataLoader ---\nclass SequenceDataset(Dataset):\n    def __init__(self, X, Y):\n        # Move inputs to device and set dtype\n        self.X = torch.from_numpy(X).to(device=device, dtype=torch.float32)\n        # Move targets to device, set dtype, and unsqueeze to [batch, seq, 1] for loss calculation\n        self.Y = torch.from_numpy(Y).to(device=device, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Since the DataLoader batch_size is the full dataset size, \n        # this will effectively return the entire X and Y tensors on the first call.\n        return self.X[idx], self.Y[idx]\n\n# Create DataLoader\ndataset = SequenceDataset(inputs, targets)\n# DataLoader batch_size is n_batch, so we get one large batch: (1, n_batch, n_seq, n_features)\ndataloader = DataLoader(dataset, batch_size=n_batch, shuffle=False) \n\n# --- 3. Model Definition and Xavier Initialization ---\n\nclass SimpleRNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(SimpleRNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Using GRU as in the previous example\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n        self._init_weights()\n\n    def _init_weights(self):\n        # Xavier/Glorot Initialization\n        for name, param in self.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_uniform_(param)\n            elif 'bias' in name:\n                nn.init.constant_(param, 1)\n        print(\"Model weights initialized with Xavier/Glorot.\")\n\n    def forward(self, x):\n        # out: (batch_size, n_seq, hidden_size) - Sequence of hidden states\n        out, _ = self.rnn(x)  \n        \n        # final_out: (batch_size, n_seq, output_size) - Final prediction\n        final_out = self.fc(out)\n        \n        # Return both prediction and hidden states for regularization\n        return final_out, out \n\n# Instantiate the model\nmodel = SimpleRNNModel(\n    input_size=n_features, \n    hidden_size=n_hidden, \n    num_layers=n_layers, \n    output_size=1\n).to(device) # Ensure model is also on the device\n\n# --- 4. Loss and Optimizer ---\n\n# SmoothL1Loss for robustness\ncriterion_primary = nn.MSELoss().to(device) \n# AdamW Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n\n# --- 5. Training Loop ---\n\nprint(\"-\" * 30)\nprint(f\"Starting Training for {n_epochs} epochs...\")\nmodel.train() \n\nfor epoch in range(1, n_epochs + 1):\n    epoch_loss = 0\n    \n    for inputs_batch, targets_batch in dataloader:\n        # Data is already on the device due to the Dataset implementation\n        # Forward pass:\n        outputs, hidden_states = model(inputs_batch)\n        \n        # 1. Primary Loss (Smooth L1 Loss)\n        primary_loss = criterion_primary(outputs[:,-1,-1], targets_batch)\n\n        # 2. Hidden State Regularization Loss (L2 Norm Squared)\n        hidden_reg_loss = torch.norm(hidden_states, p=2)**2 * HIDDEN_REG_ALPHA\n        \n        # 3. Total Loss\n        loss = primary_loss + hidden_reg_loss\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n\n    # Reporting every 1000 epochs to avoid excessive output\n    if epoch % 100 == 0 or epoch == 1:\n        avg_loss = epoch_loss / len(dataloader)\n        print(f'Epoch [{epoch:05d}/{n_epochs}], Total Loss: {avg_loss:.6f} (Primary Loss: {primary_loss.item():.6f}, Reg Loss: {hidden_reg_loss.item():.6f})')\n\nprint(\"-\" * 30)\nprint(\"Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:37:54.459604Z","iopub.execute_input":"2025-11-24T12:37:54.459887Z","iopub.status.idle":"2025-11-24T12:38:11.681502Z","shell.execute_reply.started":"2025-11-24T12:37:54.459865Z","shell.execute_reply":"2025-11-24T12:38:11.680414Z"}},"outputs":[{"name":"stdout","text":"--- Configuration Summary ---\nData Shape (Batch, Seq, Feat): (39, 240, 6)\nHidden Size/Layers: 4/1\nLearning Rate/Epochs: 0.1/1000\nUsing Device: cpu\n-----------------------------\nModel weights initialized with Xavier/Glorot.\n------------------------------\nStarting Training for 1000 epochs...\nEpoch [00001/1000], Total Loss: 60.358364 (Primary Loss: 60.358360, Reg Loss: 0.000002)\nEpoch [00100/1000], Total Loss: 4.114655 (Primary Loss: 2.246572, Reg Loss: 1.868084)\nEpoch [00200/1000], Total Loss: 4.115640 (Primary Loss: 2.247556, Reg Loss: 1.868084)\nEpoch [00300/1000], Total Loss: 4.115234 (Primary Loss: 2.247150, Reg Loss: 1.868084)\nEpoch [00400/1000], Total Loss: 4.115030 (Primary Loss: 2.246946, Reg Loss: 1.868084)\nEpoch [00500/1000], Total Loss: 4.114907 (Primary Loss: 2.246824, Reg Loss: 1.868084)\nEpoch [00600/1000], Total Loss: 4.114826 (Primary Loss: 2.246742, Reg Loss: 1.868084)\nEpoch [00700/1000], Total Loss: 4.114769 (Primary Loss: 2.246685, Reg Loss: 1.868084)\nEpoch [00800/1000], Total Loss: 4.114726 (Primary Loss: 2.246642, Reg Loss: 1.868084)\nEpoch [00900/1000], Total Loss: 4.114693 (Primary Loss: 2.246608, Reg Loss: 1.868084)\nEpoch [01000/1000], Total Loss: 4.114666 (Primary Loss: 2.246583, Reg Loss: 1.868084)\n------------------------------\nTraining Complete!\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"## Learning a time-series -> yield function\n\nHere we're very data-limited using only a single location. There are 240x4+1 inputs, but only 39 training examples. \n\n1. DNN with dropout (for generalisation). With a VAE-like dimension reduction layer-by-layer.\n\nOne idea would be to do a convolution over the 240x4 inputs and reduce them to an N dimensional signal, which is then combined with the C02 data to produce a yield estimate. This might not be crazy at all.\n\n","metadata":{}},{"cell_type":"code","source":"# DNN with dropout\n# simply map continous data onto the yield with an N-deep network trained with dropout.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# --- 2. PyTorch Dataset & DataLoader (Updated for 1D target) ---\nclass FlattenedDataset(Dataset):\n    def __init__(self, climate_data, soil_co2, crop_yield):\n        \"\"\"Takes numpy arrays as inputs.\n        climate_data shaped as (n_batch, n_seq, n_features)\n        soil_co2 and crop_yield both 1d arrays\"\"\"\n        N, S, F = climate_data.shape\n        # X: Flattened to [N, S * F]\n        self.X = torch.from_numpy(climate_data).reshape(N, S * F)\n        self.X = torch.concat([self.X,torch.from_numpy(soil_co2).unsqueeze(-1)],axis=1).to(device, dtype=torch.float32)\n        # Y: Unsqueeze to [N, 1] for consistent loss calculation\n        self.Y = torch.from_numpy(crop_yield).unsqueeze(-1).to(device=device, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.Y[idx]\n\n# --- 3. Flexible Model Definition and Xavier Initialization (DNN) ---\n\nclass FlexibleFlattenedDNN(nn.Module):\n    def __init__(self, input_size, output_size, num_layers, reduction_ratio, dropout_rate=0.0, leaky_relu_slope =0.01):\n        super(FlexibleFlattenedDNN, self).__init__()\n        self.leaky_slope = leaky_relu_slope\n        layers = []\n        current_size = input_size\n        \n        # Dynamically build the hidden layers\n        for i in range(num_layers):\n            # Calculate the size of the next layer\n            next_size = max(4, int(current_size * reduction_ratio)) # Min size of 4 for stability\n            \n            # Add Linear Layer\n            layers.append(nn.Linear(current_size, next_size))\n            # Add Activation\n            layers.append(nn.LeakyReLU(self.leaky_slope))\n            \n            # Add Dropout (only for intermediate layers)\n            if dropout_rate > 0 and i < num_layers - 1:\n                 layers.append(nn.Dropout(dropout_rate)) \n                \n            current_size = next_size\n        \n        # Add the final output layer (no activation or dropout after this)\n        layers.append(nn.Linear(current_size, output_size))\n        \n        self.fc_stack = nn.Sequential(*layers)\n        \n        print(f\"DNN Architecture built: {input_size} -> {[l.out_features for l in layers if isinstance(l, nn.Linear)]}\")\n        \n        self._init_weights()\n\n    def _init_weights(self):\n        # Xavier/Glorot Initialization for all Linear layers\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                #nn.init.xavier_uniform_(m.weight)\n                nn.init.kaiming_normal_(m.weight, a= self.leaky_slope)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        print(\"Model weights initialized with Xavier/Glorot.\")\n\n    def forward(self, x):\n        return self.fc_stack(x) \n\n# --- 1. Configuration ---\nn_batch = sequenced_data.shape[0] \nn_seq = sequenced_data.shape[1]   \nn_features = sequenced_data.shape[2] - 1\n\n# --- MODEL FLEXIBILITY PARAMETERS ---\nNUM_HIDDEN_LAYERS = 3     # The number of layers between input and output (e.g., 3 for 4 layers total)\nREDUCTION_RATIO = 1/6      # The ratio by which each layer size decreases (e.g., 0.5 means half the size)\nDROPOUT_RATE = 2/5        # Dropout rate for intermediate layers (0.0 for no dropout)\n\n# --- CALCULATED DIMENSIONS ---\nFLATTENED_INPUT_SIZE = n_seq * 4+1 \nTARGET_OUTPUT_SIZE = 1 # Corrected to 1D output\n\n# Hyperparameter\ninit_LR = 3e-9\nmax_LR = 9e-3\nweight_decay = 1e-5 #suggested to be smaller for 'super-convergence' in OneCycleLR paper.\nn_epochs = 5000\n\n\n# Create DataLoader\ndataset = FlattenedDataset(climate_data, static_data.co2.values,static_data['yield'].values)\nn_train = int(dataset.X.shape[0]*0.8)\nn_test = dataset.X.shape[0]-n_train\n\ntrain_dataset = torch.utils.data.Subset(dataset, range(n_train))\nval_dataset = torch.utils.data.Subset(dataset,range(n_train,n_train+n_test))\n\ntrain_loader = DataLoader(train_dataset, batch_size=n_batch, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size = n_batch, shuffle = False)\n\n# Instantiate the flexible model\nmodel = FlexibleFlattenedDNN(\n    input_size=FLATTENED_INPUT_SIZE,\n    output_size=TARGET_OUTPUT_SIZE,\n    num_layers=NUM_HIDDEN_LAYERS,\n    reduction_ratio=REDUCTION_RATIO,\n    dropout_rate=DROPOUT_RATE\n).to(device, dtype=torch.float32)\n\nn_params = sum([p.numel() for p in model.parameters()])\nprint(f\"Model instantiatied with {n_params} parameters.\")\n# --- 4. Loss and Optimizer ---\n\n# Switched back to MSELoss as requested in the original code block\ncriterion_primary = nn.MSELoss().to(device) \noptimizer = optim.AdamW(model.parameters(), lr=init_LR, weight_decay = weight_decay)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_LR, \n                                          steps_per_epoch=1, \n                                          epochs=n_epochs)\n\n# --- 5. Training Loop (Unchanged) ---\n\nprint(\"-\" * 30)\nprint(f\"Starting Training for {n_epochs} epochs...\")\nmodel.train() \n\nfor epoch in range(1, n_epochs + 1):\n    for inputs_batch, targets_batch in train_loader:\n        # Backward and optimize\n        optimizer.zero_grad()\n        \n        # Forward pass:\n        outputs = model(inputs_batch)\n        \n        # Calculate loss (outputs: [N, 1], targets: [N, 1])\n        loss = criterion_primary(outputs, targets_batch)\n        \n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    if epoch % 1000 == 0 or epoch == 1:\n        avg_loss = loss \n        print(f'Epoch [{epoch:05d}/{n_epochs}], Loss: {avg_loss:.6f}')\n        model.eval()\n        with torch.no_grad():\n            val_losses = []\n            for val_inputs, val_targets in val_loader:\n                val_preds = model(val_inputs)\n                val_losses.append(criterion_primary(val_preds,val_targets).item())\n\n        print(f'Validation loss: {np.mean(val_losses):.6f}')\nprint(\"-\" * 30)\nprint(\"Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:40:58.788347Z","iopub.execute_input":"2025-11-24T12:40:58.788881Z","iopub.status.idle":"2025-11-24T12:41:15.014968Z","shell.execute_reply.started":"2025-11-24T12:40:58.788853Z","shell.execute_reply":"2025-11-24T12:41:15.013920Z"}},"outputs":[{"name":"stdout","text":"DNN Architecture built: 961 -> [160, 26, 4, 1]\nModel weights initialized with Xavier/Glorot.\nModel instantiatied with 158219 parameters.\n------------------------------\nStarting Training for 5000 epochs...\nEpoch [00001/5000], Loss: 4061.400635\nValidation loss: 98.617279\nEpoch [01000/5000], Loss: 2.279125\nValidation loss: 1.501750\nEpoch [02000/5000], Loss: 1.264415\nValidation loss: 1.392141\nEpoch [03000/5000], Loss: 0.039816\nValidation loss: 1.225056\nEpoch [04000/5000], Loss: 0.003772\nValidation loss: 1.200222\nEpoch [05000/5000], Loss: 0.002243\nValidation loss: 1.196635\n------------------------------\nTraining Complete!\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"print(val_preds.T, '\\n',val_targets.T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:37:28.038922Z","iopub.execute_input":"2025-11-24T11:37:28.039181Z","iopub.status.idle":"2025-11-24T11:37:28.046061Z","shell.execute_reply.started":"2025-11-24T11:37:28.039164Z","shell.execute_reply":"2025-11-24T11:37:28.045064Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 7.6704,  9.0087,  9.0024,  8.6160, 10.7008,  9.0340,  8.7720,  8.6591]]) \n tensor([[ 9.5290,  7.0030,  8.6340,  7.4670, 11.5860,  8.8950,  7.3440,  8.9260]])\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"for name, params in model.named_parameters():\n    print(name)\n\nmodel.fc_stack[(0)].weight[]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:24:36.181575Z","iopub.execute_input":"2025-11-24T11:24:36.181936Z","iopub.status.idle":"2025-11-24T11:24:36.192261Z","shell.execute_reply.started":"2025-11-24T11:24:36.181908Z","shell.execute_reply":"2025-11-24T11:24:36.191304Z"}},"outputs":[{"name":"stdout","text":"fc_stack.0.weight\nfc_stack.0.bias\nfc_stack.3.weight\nfc_stack.3.bias\nfc_stack.6.weight\nfc_stack.6.bias\nfc_stack.8.weight\nfc_stack.8.bias\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.0781,  0.0149, -0.0588, -0.0413, -0.0362, -0.0397,  0.0411,  0.0031,\n        -0.0608, -0.0208, -0.0488,  0.0084,  0.0646, -0.0818, -0.0814, -0.0193,\n        -0.0359, -0.0204,  0.0859, -0.0047, -0.0289, -0.0616, -0.0333,  0.0642,\n         0.0704, -0.0371,  0.0234,  0.0768,  0.0118, -0.0679,  0.0206, -0.0010,\n        -0.0605, -0.0812, -0.0428, -0.0416, -0.0118, -0.0483,  0.0239, -0.0181,\n        -0.0075, -0.0717,  0.0292,  0.0408, -0.0871, -0.0020, -0.0265, -0.0811,\n        -0.0300,  0.0800, -0.0514,  0.0386, -0.0690, -0.0437,  0.0354, -0.0230,\n        -0.0456,  0.0232, -0.0515, -0.0029, -0.0225,  0.0355,  0.0393, -0.0589,\n         0.0884, -0.0167, -0.0537,  0.0057, -0.0103, -0.0388,  0.0207,  0.0920,\n        -0.0072,  0.0334, -0.0164,  0.0446, -0.0403, -0.0480, -0.0126,  0.0709,\n        -0.0071, -0.0423, -0.0876,  0.0138, -0.0053,  0.0341,  0.0070, -0.0136,\n        -0.0116, -0.0288,  0.0078, -0.0319, -0.0338, -0.0095, -0.0179,  0.0280,\n         0.0325, -0.0699, -0.0515, -0.0069, -0.0468,  0.0197, -0.0151,  0.0110,\n        -0.0458, -0.0183, -0.0349, -0.0639,  0.0132,  0.0332,  0.0040, -0.0141,\n        -0.0294, -0.0474,  0.0046,  0.0091, -0.0380,  0.0379, -0.0103, -0.0261,\n         0.0013,  0.0327,  0.0084, -0.0385,  0.0026, -0.0303, -0.0604,  0.0270,\n        -0.0606,  0.0257,  0.0155, -0.0876,  0.0434,  0.0325,  0.0452, -0.0373,\n        -0.0462, -0.0388,  0.0197, -0.0104, -0.0578,  0.0686, -0.0190, -0.0421,\n        -0.0361,  0.0183,  0.0294, -0.0099,  0.0033, -0.0413, -0.0058,  0.0175,\n        -0.0209,  0.0879,  0.1215, -0.0464, -0.0786,  0.0192, -0.0584,  0.0864],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"print(val_preds.T, '\\n',val_targets.T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:32:28.266290Z","iopub.execute_input":"2025-11-24T11:32:28.267080Z","iopub.status.idle":"2025-11-24T11:32:28.273381Z","shell.execute_reply.started":"2025-11-24T11:32:28.267052Z","shell.execute_reply":"2025-11-24T11:32:28.272502Z"}},"outputs":[{"name":"stdout","text":"tensor([[8.6046, 8.6046, 8.6046, 8.6046, 8.6046, 8.6046, 8.6046, 8.6046]]) \n tensor([[ 9.5290,  7.0030,  8.6340,  7.4670, 11.5860,  8.8950,  7.3440,  8.9260]])\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}