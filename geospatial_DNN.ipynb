{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":81000,"databundleVersionId":8812083,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nAfter looking at the FutureCrop data, it appears that there's a large variance across locations in the mean yield and also the variance.\n\nHere we will try to model each location independently- we then will try to scale this up to train the many models in parallel on a GPU using pytorch.\n\nThe idea is to learn the minimal mapping from time-series data to a prediction of the yield.\n\n**Notes**:\nA simple recurrent model just learns to predict the mean across all training batches of a single location. This is true for tiny (1-unit), shallow (100-unit wide) and deep networks (2-4 layers).\nHowever, models taking the entire sequence of weather as a single vector learn to do pattern recognition and generalise.\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata_dir = '/kaggle/input/the-future-crop-challenge/'\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:39:09.665632Z","iopub.execute_input":"2025-12-18T21:39:09.665913Z","iopub.status.idle":"2025-12-18T21:39:09.919112Z","shell.execute_reply.started":"2025-12-18T21:39:09.665890Z","shell.execute_reply":"2025-12-18T21:39:09.918497Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/the-future-crop-challenge/pr_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/sample_submission.csv\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_maize.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/pr_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_wheat.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_train.parquet\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#climate data (timeseries)\n\n#soil_co2 and yields\n\nwheat_df = pd.read_parquet('/kaggle/input/the-future-crop-challenge/soil_co2_wheat_train.parquet')\nwheat_yield = pd.read_parquet('/kaggle/input/the-future-crop-challenge/train_solutions_wheat.parquet')\nwheat_df = wheat_df.join(wheat_yield)\n\nmaize_df = pd.read_parquet('/kaggle/input/the-future-crop-challenge/soil_co2_maize_train.parquet')\nmaize_yield = pd.read_parquet('/kaggle/input/the-future-crop-challenge/train_solutions_maize.parquet')\nmaize_df = maize_df.join(maize_yield)\n\nmean_df = pd.DataFrame()\nfor crop_df in [wheat_df,maize_df]:\n    temp_df = crop_df.groupby(['crop','lon','lat'], as_index = False).agg({'yield':['mean','std']})\n    temp_df.columns = ['crop','lon','lat','yield_mean','yield_std']\n    mean_df = pd.concat([mean_df,temp_df])\n\nmean_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T15:40:35.219161Z","iopub.execute_input":"2025-12-16T15:40:35.219645Z","iopub.status.idle":"2025-12-16T15:40:35.720373Z","shell.execute_reply.started":"2025-12-16T15:40:35.219612Z","shell.execute_reply":"2025-12-16T15:40:35.719490Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"       crop     lon    lat  yield_mean  yield_std\n0     wheat -123.25  44.75    4.965216   0.488492\n1     wheat -123.25  45.25    4.985947   0.501791\n2     wheat -123.25  45.75    4.822316   0.423168\n3     wheat -122.75  44.75    4.875486   0.537611\n4     wheat -122.75  45.25    5.379421   0.615185\n...     ...     ...    ...         ...        ...\n9298  maize  132.75  46.75    5.738692   1.191923\n9299  maize  132.75  47.25    8.622872   1.518404\n9300  maize  133.25  45.25    2.470256   0.359327\n9301  maize  133.25  47.25    6.932128   1.339325\n9302  maize  137.75  36.75    2.396718   0.298963\n\n[17966 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>crop</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>yield_mean</th>\n      <th>yield_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wheat</td>\n      <td>-123.25</td>\n      <td>44.75</td>\n      <td>4.965216</td>\n      <td>0.488492</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wheat</td>\n      <td>-123.25</td>\n      <td>45.25</td>\n      <td>4.985947</td>\n      <td>0.501791</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wheat</td>\n      <td>-123.25</td>\n      <td>45.75</td>\n      <td>4.822316</td>\n      <td>0.423168</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wheat</td>\n      <td>-122.75</td>\n      <td>44.75</td>\n      <td>4.875486</td>\n      <td>0.537611</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wheat</td>\n      <td>-122.75</td>\n      <td>45.25</td>\n      <td>5.379421</td>\n      <td>0.615185</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9298</th>\n      <td>maize</td>\n      <td>132.75</td>\n      <td>46.75</td>\n      <td>5.738692</td>\n      <td>1.191923</td>\n    </tr>\n    <tr>\n      <th>9299</th>\n      <td>maize</td>\n      <td>132.75</td>\n      <td>47.25</td>\n      <td>8.622872</td>\n      <td>1.518404</td>\n    </tr>\n    <tr>\n      <th>9300</th>\n      <td>maize</td>\n      <td>133.25</td>\n      <td>45.25</td>\n      <td>2.470256</td>\n      <td>0.359327</td>\n    </tr>\n    <tr>\n      <th>9301</th>\n      <td>maize</td>\n      <td>133.25</td>\n      <td>47.25</td>\n      <td>6.932128</td>\n      <td>1.339325</td>\n    </tr>\n    <tr>\n      <th>9302</th>\n      <td>maize</td>\n      <td>137.75</td>\n      <td>36.75</td>\n      <td>2.396718</td>\n      <td>0.298963</td>\n    </tr>\n  </tbody>\n</table>\n<p>17966 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"idx_lon = 132.75\nidx_lat = 47.25\n\nstatic_data = maize_df.query(f'lon=={idx_lon} and lat=={idx_lat}')[['co2','nitrogen','yield']]\n\ncrop = 'maize'\nmode = 'train'\n\ntasmax = pd.read_parquet(os.path.join(data_dir, f\"tasmax_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\ntasmin = pd.read_parquet(os.path.join(data_dir, f\"tasmin_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\npr = pd.read_parquet(os.path.join(data_dir, f\"pr_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\nrsds = pd.read_parquet(os.path.join(data_dir, f\"rsds_{crop}_{mode}.parquet\")).query(f'lon=={idx_lon} and lat=={idx_lat}')\n\nclimate_data = np.stack([\n        tasmax.iloc[:, 5:].values,\n        tasmin.iloc[:, 5:].values,\n        pr.iloc[:, 5:].values,\n        rsds.iloc[:, 5:].values\n    ], axis=2)\n\nstatic_expanded = np.repeat(static_data.values[:,np.newaxis,:],240,axis=1)\n\nsequenced_data = np.concatenate([climate_data,static_expanded],axis = 2)\n\nsequenced_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:28:42.610012Z","iopub.execute_input":"2025-12-16T14:28:42.610302Z","iopub.status.idle":"2025-12-16T14:28:46.566092Z","shell.execute_reply.started":"2025-12-16T14:28:42.610284Z","shell.execute_reply":"2025-12-16T14:28:46.565464Z"}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"(39, 240, 7)"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- Data Splitting ---\ninputs = sequenced_data[:, :, :-1]\ntargets = sequenced_data[:, -1, -1]\n\n# --- 1. Configuration ---\nn_batch = sequenced_data.shape[0] # Using full dataset as one batch\nn_seq = sequenced_data.shape[1]\nn_features = sequenced_data.shape[2] - 1\nn_hidden = 4\nn_layers = 1\nlearning_rate = 1e-1\nn_epochs = 1000\n\n# Regularization Hyperparameter (Adjust if needed)\nHIDDEN_REG_ALPHA = 1e-4 \n\n# Device configuration (Move all data/model to the device)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"--- Configuration Summary ---\")\nprint(f\"Data Shape (Batch, Seq, Feat): ({n_batch}, {n_seq}, {n_features})\")\nprint(f\"Hidden Size/Layers: {n_hidden}/{n_layers}\")\nprint(f\"Learning Rate/Epochs: {learning_rate}/{n_epochs}\")\nprint(f\"Using Device: {device}\")\nprint(f\"-----------------------------\")\n\n\n# --- 2. PyTorch Dataset & DataLoader ---\nclass SequenceDataset(Dataset):\n    def __init__(self, X, Y):\n        # Move inputs to device and set dtype\n        self.X = torch.from_numpy(X).to(device=device, dtype=torch.float32)\n        # Move targets to device, set dtype, and unsqueeze to [batch, seq, 1] for loss calculation\n        self.Y = torch.from_numpy(Y).to(device=device, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Since the DataLoader batch_size is the full dataset size, \n        # this will effectively return the entire X and Y tensors on the first call.\n        return self.X[idx], self.Y[idx]\n\n# Create DataLoader\ndataset = SequenceDataset(inputs, targets)\n# DataLoader batch_size is n_batch, so we get one large batch: (1, n_batch, n_seq, n_features)\ndataloader = DataLoader(dataset, batch_size=n_batch, shuffle=False) \n\n# --- 3. Model Definition and Xavier Initialization ---\n\nclass SimpleRNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(SimpleRNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Using GRU as in the previous example\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n        self._init_weights()\n\n    def _init_weights(self):\n        # Xavier/Glorot Initialization\n        for name, param in self.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_uniform_(param)\n            elif 'bias' in name:\n                nn.init.constant_(param, 1)\n        print(\"Model weights initialized with Xavier/Glorot.\")\n\n    def forward(self, x):\n        # out: (batch_size, n_seq, hidden_size) - Sequence of hidden states\n        out, _ = self.rnn(x)  \n        \n        # final_out: (batch_size, n_seq, output_size) - Final prediction\n        final_out = self.fc(out)\n        \n        # Return both prediction and hidden states for regularization\n        return final_out, out \n\n# Instantiate the model\nmodel = SimpleRNNModel(\n    input_size=n_features, \n    hidden_size=n_hidden, \n    num_layers=n_layers, \n    output_size=1\n).to(device) # Ensure model is also on the device\n\n# --- 4. Loss and Optimizer ---\n\n# SmoothL1Loss for robustness\ncriterion_primary = nn.MSELoss().to(device) \n# AdamW Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n\n# --- 5. Training Loop ---\n\nprint(\"-\" * 30)\nprint(f\"Starting Training for {n_epochs} epochs...\")\nmodel.train() \n\nfor epoch in range(1, n_epochs + 1):\n    epoch_loss = 0\n    \n    for inputs_batch, targets_batch in dataloader:\n        # Data is already on the device due to the Dataset implementation\n        # Forward pass:\n        outputs, hidden_states = model(inputs_batch)\n        \n        # 1. Primary Loss (Smooth L1 Loss)\n        primary_loss = criterion_primary(outputs[:,-1,-1], targets_batch)\n\n        # 2. Hidden State Regularization Loss (L2 Norm Squared)\n        hidden_reg_loss = torch.norm(hidden_states, p=2)**2 * HIDDEN_REG_ALPHA\n        \n        # 3. Total Loss\n        loss = primary_loss + hidden_reg_loss\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n\n    # Reporting every 1000 epochs to avoid excessive output\n    if epoch % 100 == 0 or epoch == 1:\n        avg_loss = epoch_loss / len(dataloader)\n        print(f'Epoch [{epoch:05d}/{n_epochs}], Total Loss: {avg_loss:.6f} (Primary Loss: {primary_loss.item():.6f}, Reg Loss: {hidden_reg_loss.item():.6f})')\n\nprint(\"-\" * 30)\nprint(\"Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:10:59.518478Z","iopub.execute_input":"2025-12-16T14:10:59.518783Z","iopub.status.idle":"2025-12-16T14:11:09.320526Z","shell.execute_reply.started":"2025-12-16T14:10:59.518757Z","shell.execute_reply":"2025-12-16T14:11:09.319850Z"}},"outputs":[{"name":"stdout","text":"--- Configuration Summary ---\nData Shape (Batch, Seq, Feat): (39, 240, 6)\nHidden Size/Layers: 4/1\nLearning Rate/Epochs: 0.1/1000\nUsing Device: cuda\n-----------------------------\nModel weights initialized with Xavier/Glorot.\n------------------------------\nStarting Training for 1000 epochs...\nEpoch [00001/1000], Total Loss: 60.354610 (Primary Loss: 60.354610, Reg Loss: 0.000000)\nEpoch [00100/1000], Total Loss: 3.373423 (Primary Loss: 3.373423, Reg Loss: 0.000000)\nEpoch [00200/1000], Total Loss: 2.311594 (Primary Loss: 2.311594, Reg Loss: 0.000000)\nEpoch [00300/1000], Total Loss: 2.285419 (Primary Loss: 2.285419, Reg Loss: 0.000000)\nEpoch [00400/1000], Total Loss: 2.273779 (Primary Loss: 2.273779, Reg Loss: 0.000000)\nEpoch [00500/1000], Total Loss: 2.267073 (Primary Loss: 2.267073, Reg Loss: 0.000000)\nEpoch [00600/1000], Total Loss: 2.262702 (Primary Loss: 2.262702, Reg Loss: 0.000000)\nEpoch [00700/1000], Total Loss: 2.259630 (Primary Loss: 2.259630, Reg Loss: 0.000000)\nEpoch [00800/1000], Total Loss: 2.257360 (Primary Loss: 2.257360, Reg Loss: 0.000000)\nEpoch [00900/1000], Total Loss: 2.255619 (Primary Loss: 2.255619, Reg Loss: 0.000000)\nEpoch [01000/1000], Total Loss: 2.254247 (Primary Loss: 2.254247, Reg Loss: 0.000000)\n------------------------------\nTraining Complete!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Learning a time-series -> yield function\n\nHere we're very data-limited using only a single location. There are 240x4+1 inputs, but only 39 training examples. \n\n1. DNN with dropout (for generalisation). With a VAE-like dimension reduction layer-by-layer.\n\nOne idea would be to do a convolution over the 240x4 inputs and reduce them to an N dimensional signal, which is then combined with the C02 data to produce a yield estimate. This might not be crazy at all.\n\n","metadata":{}},{"cell_type":"code","source":"# DNN with dropout\n# simply map continous data onto the yield with an N-deep network trained with dropout.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# --- 2. PyTorch Dataset & DataLoader (Updated for 1D target) ---\nclass FlattenedDataset(Dataset):\n    def __init__(self, climate_data, soil_co2, crop_yield):\n        \"\"\"Takes numpy arrays as inputs.\n        climate_data shaped as (n_batch, n_seq, n_features)\n        soil_co2 and crop_yield both 1d arrays\"\"\"\n        N, S, F = climate_data.shape\n        # X: Flattened to [N, S * F]\n        self.X = torch.from_numpy(climate_data).reshape(N, S * F)\n        self.X = torch.concat([self.X,torch.from_numpy(soil_co2).unsqueeze(-1)],axis=1).to(device, dtype=torch.float32)\n        # Y: Unsqueeze to [N, 1] for consistent loss calculation\n        self.Y = torch.from_numpy(crop_yield).unsqueeze(-1).to(device=device, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.Y[idx]\n\n# --- 3. Flexible Model Definition and Xavier Initialization (DNN) ---\n\nclass FlexibleFlattenedDNN(nn.Module):\n    def __init__(self, input_size, output_size, num_layers, reduction_ratio, dropout_rate=0.0, leaky_relu_slope =0.01):\n        super(FlexibleFlattenedDNN, self).__init__()\n        self.leaky_slope = leaky_relu_slope\n        layers = []\n        current_size = input_size\n        \n        # Dynamically build the hidden layers\n        for i in range(num_layers):\n            # Calculate the size of the next layer\n            next_size = max(4, int(current_size * reduction_ratio)) # Min size of 4 for stability\n            \n            # Add Linear Layer\n            layers.append(nn.Linear(current_size, next_size))\n            # Add Activation\n            layers.append(nn.LeakyReLU(self.leaky_slope))\n            \n            # Add Dropout (only for intermediate layers)\n            if dropout_rate > 0 and i < num_layers - 1:\n                 layers.append(nn.Dropout(dropout_rate)) \n                \n            current_size = next_size\n        \n        # Add the final output layer (no activation or dropout after this)\n        layers.append(nn.Linear(current_size, output_size))\n        \n        self.fc_stack = nn.Sequential(*layers)\n        \n        print(f\"DNN Architecture built: {input_size} -> {[l.out_features for l in layers if isinstance(l, nn.Linear)]}\")\n        \n        self._init_weights()\n\n    def _init_weights(self, weight_seed = 1):\n        # Xavier/Glorot Initialization for all Linear layers\n        torch.random.seed = weight_seed\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                #nn.init.xavier_uniform_(m.weight)\n                nn.init.kaiming_normal_(m.weight, a= self.leaky_slope)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        print(\"Model weights initialized with Xavier/Glorot.\")\n\n    def forward(self, x):\n        return self.fc_stack(x) \n\n# --- 1. Configuration ---\nn_batch = sequenced_data.shape[0] \nn_seq = sequenced_data.shape[1]   \nn_features = sequenced_data.shape[2] - 1\n\n# --- MODEL FLEXIBILITY PARAMETERS ---\nNUM_HIDDEN_LAYERS = 3     # The number of layers between input and output (e.g., 3 for 4 layers total)\nREDUCTION_RATIO = 1/6      # The ratio by which each layer size decreases (e.g., 0.5 means half the size)\nDROPOUT_RATE = 2/5        # Dropout rate for intermediate layers (0.0 for no dropout)\n\n# --- CALCULATED DIMENSIONS ---\nFLATTENED_INPUT_SIZE = n_seq * 4+1 \nTARGET_OUTPUT_SIZE = 1 # Corrected to 1D output\n\n# Hyperparameter\ninit_LR = 3e-9\nmax_LR = 9e-3\nweight_decay = 1e-5 #suggested to be smaller for 'super-convergence' in OneCycleLR paper.\nn_epochs = 1000\n\n\n# Create DataLoader\ndataset = FlattenedDataset(climate_data, static_data.co2.values,static_data['yield'].values)\nn_train = int(dataset.X.shape[0]*0.8)\nn_test = dataset.X.shape[0]-n_train\n\ntrain_dataset = torch.utils.data.Subset(dataset, range(n_train))\nval_dataset = torch.utils.data.Subset(dataset,range(n_train,n_train+n_test))\n\ntrain_loader = DataLoader(train_dataset, batch_size=n_batch, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size = n_batch, shuffle = False)\n\n# Instantiate the flexible model\nmodel = FlexibleFlattenedDNN(\n    input_size=FLATTENED_INPUT_SIZE,\n    output_size=TARGET_OUTPUT_SIZE,\n    num_layers=NUM_HIDDEN_LAYERS,\n    reduction_ratio=REDUCTION_RATIO,\n    dropout_rate=DROPOUT_RATE\n).to(device, dtype=torch.float32)\n\nn_params = sum([p.numel() for p in model.parameters()])\nprint(f\"Model instantiatied with {n_params} parameters.\")\n# --- 4. Loss and Optimizer ---\n\n# Switched back to MSELoss as requested in the original code block\ncriterion_primary = nn.MSELoss().to(device) \noptimizer = optim.AdamW(model.parameters(), lr=init_LR, weight_decay = weight_decay)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_LR, \n                                          steps_per_epoch=1, \n                                          epochs=n_epochs)\n\n# --- 5. Training Loop (Unchanged) ---\n\nprint(\"-\" * 30)\nprint(f\"Starting Training for {n_epochs} epochs...\")\nmodel.train() \n\nfor epoch in range(1, n_epochs + 1):\n    for inputs_batch, targets_batch in train_loader:\n        # Backward and optimize\n        optimizer.zero_grad()\n        \n        # Forward pass:\n        outputs = model(inputs_batch)\n        \n        # Calculate loss (outputs: [N, 1], targets: [N, 1])\n        loss = criterion_primary(outputs, targets_batch)\n        \n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    if epoch % 1000 == 0 or epoch == 1:\n        avg_loss = loss \n        print(f'Epoch [{epoch:05d}/{n_epochs}], Loss: {avg_loss:.6f}')\n        model.eval()\n        with torch.no_grad():\n            val_losses = []\n            for val_inputs, val_targets in val_loader:\n                val_preds = model(val_inputs)\n                val_losses.append(criterion_primary(val_preds,val_targets).item())\n\n        print(f'Validation loss: {np.mean(val_losses):.6f}')\nprint(\"-\" * 30)\nprint(\"Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:23:52.372152Z","iopub.execute_input":"2025-12-16T14:23:52.372762Z","iopub.status.idle":"2025-12-16T14:23:54.311014Z","shell.execute_reply.started":"2025-12-16T14:23:52.372739Z","shell.execute_reply":"2025-12-16T14:23:54.310201Z"}},"outputs":[{"name":"stdout","text":"DNN Architecture built: 961 -> [160, 26, 4, 1]\nModel weights initialized with Xavier/Glorot.\nModel instantiatied with 158219 parameters.\n------------------------------\nStarting Training for 1000 epochs...\nEpoch [00001/1000], Loss: 16278.957031\nValidation loss: 1314.556519\nEpoch [01000/1000], Loss: 0.000000\nValidation loss: 5.174515\n------------------------------\nTraining Complete!\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# the predictions are no longer constant means for every timepont, but are matching patterns.\n# this suggests that information is being pulled out of the sequence.\nprint(val_preds.T, '\\n',val_targets.T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:24:26.136034Z","iopub.execute_input":"2025-12-16T14:24:26.136736Z","iopub.status.idle":"2025-12-16T14:24:26.143964Z","shell.execute_reply.started":"2025-12-16T14:24:26.136709Z","shell.execute_reply":"2025-12-16T14:24:26.143189Z"}},"outputs":[{"name":"stdout","text":"tensor([[11.6607,  8.7769,  9.5265,  5.4197,  7.2092,  9.7951,  9.5484,  6.9533]],\n       device='cuda:0') \n tensor([[ 9.5290,  7.0030,  8.6340,  7.4670, 11.5860,  8.8950,  7.3440,  8.9260]],\n       device='cuda:0')\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"for name, params in model.named_parameters():\n    print(name)\n\nmodel.fc_stack[(0)].weight[]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:24:36.181575Z","iopub.execute_input":"2025-11-24T11:24:36.181936Z","iopub.status.idle":"2025-11-24T11:24:36.192261Z","shell.execute_reply.started":"2025-11-24T11:24:36.181908Z","shell.execute_reply":"2025-11-24T11:24:36.191304Z"}},"outputs":[{"name":"stdout","text":"fc_stack.0.weight\nfc_stack.0.bias\nfc_stack.3.weight\nfc_stack.3.bias\nfc_stack.6.weight\nfc_stack.6.bias\nfc_stack.8.weight\nfc_stack.8.bias\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.0781,  0.0149, -0.0588, -0.0413, -0.0362, -0.0397,  0.0411,  0.0031,\n        -0.0608, -0.0208, -0.0488,  0.0084,  0.0646, -0.0818, -0.0814, -0.0193,\n        -0.0359, -0.0204,  0.0859, -0.0047, -0.0289, -0.0616, -0.0333,  0.0642,\n         0.0704, -0.0371,  0.0234,  0.0768,  0.0118, -0.0679,  0.0206, -0.0010,\n        -0.0605, -0.0812, -0.0428, -0.0416, -0.0118, -0.0483,  0.0239, -0.0181,\n        -0.0075, -0.0717,  0.0292,  0.0408, -0.0871, -0.0020, -0.0265, -0.0811,\n        -0.0300,  0.0800, -0.0514,  0.0386, -0.0690, -0.0437,  0.0354, -0.0230,\n        -0.0456,  0.0232, -0.0515, -0.0029, -0.0225,  0.0355,  0.0393, -0.0589,\n         0.0884, -0.0167, -0.0537,  0.0057, -0.0103, -0.0388,  0.0207,  0.0920,\n        -0.0072,  0.0334, -0.0164,  0.0446, -0.0403, -0.0480, -0.0126,  0.0709,\n        -0.0071, -0.0423, -0.0876,  0.0138, -0.0053,  0.0341,  0.0070, -0.0136,\n        -0.0116, -0.0288,  0.0078, -0.0319, -0.0338, -0.0095, -0.0179,  0.0280,\n         0.0325, -0.0699, -0.0515, -0.0069, -0.0468,  0.0197, -0.0151,  0.0110,\n        -0.0458, -0.0183, -0.0349, -0.0639,  0.0132,  0.0332,  0.0040, -0.0141,\n        -0.0294, -0.0474,  0.0046,  0.0091, -0.0380,  0.0379, -0.0103, -0.0261,\n         0.0013,  0.0327,  0.0084, -0.0385,  0.0026, -0.0303, -0.0604,  0.0270,\n        -0.0606,  0.0257,  0.0155, -0.0876,  0.0434,  0.0325,  0.0452, -0.0373,\n        -0.0462, -0.0388,  0.0197, -0.0104, -0.0578,  0.0686, -0.0190, -0.0421,\n        -0.0361,  0.0183,  0.0294, -0.0099,  0.0033, -0.0413, -0.0058,  0.0175,\n        -0.0209,  0.0879,  0.1215, -0.0464, -0.0786,  0.0192, -0.0584,  0.0864],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"print(val_preds.T, '\\n',val_targets.T)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:32:28.266290Z","iopub.execute_input":"2025-11-24T11:32:28.267080Z","iopub.status.idle":"2025-11-24T11:32:28.273381Z","shell.execute_reply.started":"2025-11-24T11:32:28.267052Z","shell.execute_reply":"2025-11-24T11:32:28.272502Z"}},"outputs":[{"name":"stdout","text":"tensor([[8.6046, 8.6046, 8.6046, 8.6046, 8.6046, 8.6046, 8.6046, 8.6046]]) \n tensor([[ 9.5290,  7.0030,  8.6340,  7.4670, 11.5860,  8.8950,  7.3440,  8.9260]])\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"## submission\n\nLet's try running this to submission.\n\nStart with a dataframe that we can index out by crop and position, then train on all train data and evaluate on the test data.\n\n","metadata":{}},{"cell_type":"code","source":"submission_csv = pd.read_csv(data_dir+'sample_submission.csv')\n\nsubmission_csv #pandas dataframe with 'ID' and 'yield' column\nsubmission_csv.index = submission_csv['ID']\nsubmission_csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:39:23.280617Z","iopub.execute_input":"2025-12-18T21:39:23.281450Z","iopub.status.idle":"2025-12-18T21:39:23.935665Z","shell.execute_reply.started":"2025-12-18T21:39:23.281422Z","shell.execute_reply":"2025-12-18T21:39:23.935014Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"              ID      yield\nID                         \n349719    349719   1.586834\n349720    349720  16.060235\n349721    349721   3.783943\n349722    349722  16.078949\n349723    349723  18.245241\n...          ...        ...\n1873717  1873717  13.696562\n1873718  1873718  18.601414\n1873719  1873719   7.305126\n1873720  1873720  12.617616\n1873721  1873721  19.170951\n\n[1245149 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>yield</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>349719</th>\n      <td>349719</td>\n      <td>1.586834</td>\n    </tr>\n    <tr>\n      <th>349720</th>\n      <td>349720</td>\n      <td>16.060235</td>\n    </tr>\n    <tr>\n      <th>349721</th>\n      <td>349721</td>\n      <td>3.783943</td>\n    </tr>\n    <tr>\n      <th>349722</th>\n      <td>349722</td>\n      <td>16.078949</td>\n    </tr>\n    <tr>\n      <th>349723</th>\n      <td>349723</td>\n      <td>18.245241</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1873717</th>\n      <td>1873717</td>\n      <td>13.696562</td>\n    </tr>\n    <tr>\n      <th>1873718</th>\n      <td>1873718</td>\n      <td>18.601414</td>\n    </tr>\n    <tr>\n      <th>1873719</th>\n      <td>1873719</td>\n      <td>7.305126</td>\n    </tr>\n    <tr>\n      <th>1873720</th>\n      <td>1873720</td>\n      <td>12.617616</td>\n    </tr>\n    <tr>\n      <th>1873721</th>\n      <td>1873721</td>\n      <td>19.170951</td>\n    </tr>\n  </tbody>\n</table>\n<p>1245149 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# we want to end up with a dataframe that is flattened anyways. So we can still use a pandas dataframe that is just concatenated\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Device configuration\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using torch on  ', DEVICE)\n\ndata_dict = {} #containing pytorch dataframes on device - for training, the yield is appended as the last index\nidx_dict = {} #containing location to idx dictionaries\nID_dict = {} #containing location to ID dictionaries (primarily for submission alignment)\n\nfor test_or_train in ['train','test']:\n    data_dict[test_or_train] = {}\n    idx_dict[test_or_train] = {}\n    ID_dict[test_or_train] = {}\n    for each_crop in ['maize','wheat']:\n        #Load all the data first:\n        static_data = pd.read_parquet(data_dir+f'soil_co2_{each_crop}_{test_or_train}.parquet')[['year','lon','lat','co2','nitrogen']]\n        if test_or_train == 'train':\n            yield_df = pd.read_parquet(data_dir+f'train_solutions_{each_crop}.parquet')\n            static_data['yield'] = yield_df['yield']\n            print('appended yield')\n            del yield_df\n        \n        climate_data = []\n        for data_type in ['tasmax','tasmin','pr','rsds']:\n            climate_df = pd.read_parquet(data_dir+f'{data_type}_{each_crop}_{test_or_train}.parquet')\n            climate_df = climate_df.drop(columns = ['crop','year','lon','lat','variable'])\n            climate_df.columns = [f'{data_type}_{x}' for x in climate_df.keys()]\n            climate_data.append(climate_df)\n\n        # this is a useful dataframe with all the data:\n        df = pd.concat(climate_data+[static_data],axis=1) \n        X = torch.from_numpy(df.drop(columns = ['year','lon','lat']).values).to(device=DEVICE, dtype = torch.float32) #(n_years, n_features)\n        data_dict[test_or_train][each_crop] = X\n        ID_dict[test_or_train][each_crop] = df.groupby(['lon','lat']).groups\n        df.index = range(len(df)) #reindex to get a dictionary over indices\n        idx_dict[test_or_train][each_crop] = df.groupby(['lon','lat']).groups\n        #manage memory\n        del static_data, climate_data, climate_df, df, X\n        gc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:42:57.956181Z","iopub.execute_input":"2025-12-18T21:42:57.956460Z","iopub.status.idle":"2025-12-18T21:44:33.601789Z","shell.execute_reply.started":"2025-12-18T21:42:57.956437Z","shell.execute_reply":"2025-12-18T21:44:33.601100Z"}},"outputs":[{"name":"stdout","text":"Using torch on   cuda\nappended yield\nappended yield\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"## self-contained code here for training \n\n## define model\n\nclass FlexibleFlattenedDNN(nn.Module):\n    def __init__(self, input_size, output_size, num_layers, reduction_ratio, dropout_rate=0.0, leaky_relu_slope =0.01):\n        super(FlexibleFlattenedDNN, self).__init__()\n        self.leaky_slope = leaky_relu_slope\n        layers = []\n        current_size = input_size\n        \n        # Dynamically build the hidden layers\n        for i in range(num_layers):\n            # Calculate the size of the next layer\n            next_size = max(4, int(current_size * reduction_ratio)) # Min size of 4 for stability\n            \n            # Add Linear Layer\n            layers.append(nn.Linear(current_size, next_size))\n            # Add Activation\n            layers.append(nn.LeakyReLU(self.leaky_slope))\n            \n            # Add Dropout (only for intermediate layers)\n            if dropout_rate > 0 and i < num_layers - 1:\n                 layers.append(nn.Dropout(dropout_rate)) \n                \n            current_size = next_size\n        \n        # Add the final output layer (no activation or dropout after this)\n        layers.append(nn.Linear(current_size, output_size))\n        \n        self.fc_stack = nn.Sequential(*layers)\n        \n        #print(f\"DNN Architecture built: {input_size} -> {[l.out_features for l in layers if isinstance(l, nn.Linear)]}\")\n        \n        self._init_weights()\n\n    def _init_weights(self, weight_seed = 1):\n        # Xavier/Glorot Initialization for all Linear layers\n        torch.random.seed = weight_seed\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                #nn.init.xavier_uniform_(m.weight)\n                nn.init.kaiming_normal_(m.weight, a= self.leaky_slope)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        #print(\"Model weights initialized with Xavier/Glorot.\")\n\n    def forward(self, x):\n        return self.fc_stack(x) \n\n# --- MODEL FLEXIBILITY PARAMETERS ---\nNUM_HIDDEN_LAYERS = 3     # The number of layers between input and output (e.g., 3 for 4 layers total)\nREDUCTION_RATIO = 1/6      # The ratio by which each layer size decreases (e.g., 0.5 means half the size)\nDROPOUT_RATE = 2/5        # Dropout rate for intermediate layers (0.0 for no dropout)\n\n# Hyperparameters\ninit_LR = 3e-9\nmax_LR = 9e-3\nweight_decay = 1e-5 #suggested to be smaller for 'super-convergence' in OneCycleLR paper.\nn_epochs = 1000\n\n\n#\ndef train_and_predict(train_data, test_data):\n    # move data to device\n    train_X = train_data[:,:-1] #(n_years, n_features)\n    train_Y = train_data[:,-1].unsqueeze(-1)\n    test_X = test_data\n    \n    # Instantiate the flexible model\n    model = FlexibleFlattenedDNN(\n        input_size=train_X.shape[1],\n        output_size=1,\n        num_layers=NUM_HIDDEN_LAYERS,\n        reduction_ratio=REDUCTION_RATIO,\n        dropout_rate=DROPOUT_RATE\n    ).to(DEVICE, dtype=torch.float32)\n    \n    # Switched back to MSELoss as requested in the original code block\n    criterion_primary = nn.MSELoss().to(DEVICE) \n    optimizer = optim.AdamW(model.parameters(), lr=init_LR, weight_decay = weight_decay)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_LR,epochs = n_epochs, steps_per_epoch=1)\n             \n    model.train() \n    \n    for epoch in range(1, n_epochs + 1):\n            # Backward and optimize\n            optimizer.zero_grad() \n            # Forward pass:\n            outputs = model(train_X)\n            # Calculate loss (outputs: [N, 1], targets: [N, 1])\n            loss = criterion_primary(outputs, train_Y)\n            # backprop losses\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n    #print(f'Finished training {n_epochs}')\n    model.eval()\n    with torch.no_grad():\n        predictions = model(test_X)\n\n    return predictions.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:52:24.855609Z","iopub.execute_input":"2025-12-18T21:52:24.856261Z","iopub.status.idle":"2025-12-18T21:52:24.866569Z","shell.execute_reply.started":"2025-12-18T21:52:24.856235Z","shell.execute_reply":"2025-12-18T21:52:24.865814Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from tqdm import tqdm\n# with pandas dataframes it takes ~2x40 minutes just to go through all the data.\n# not ideal\nfor each_crop in ['maize','wheat']:\n    unique_coords = idx_dict['test'][each_crop].keys()\n    for lon_lat in tqdm(unique_coords):\n        #load the train_data and train a model\n        train_data = data_dict['train'][each_crop][idx_dict['train'][each_crop][lon_lat]]\n        test_data = data_dict['test'][each_crop][idx_dict['test'][each_crop][lon_lat]]\n        #run code to train and predict\n        predictions = train_and_predict(train_data,test_data)\n        #append to the submission csv\n        location_indices = ID_dict['test'][each_crop][lon_lat]        \n        submission_csv.loc[location_indices,'yield'] = predictions\n\n# I think a better / ideal indexing approach is one that loads all the data onto pytorch tensors and indexes out from there.\n# we just need some kind of location to index dictionary/look up table to index out of.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:52:27.880615Z","iopub.execute_input":"2025-12-18T21:52:27.880915Z"}},"outputs":[{"name":"stderr","text":"  1%|          | 91/9303 [02:20<3:55:39,  1.53s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"test_data= pandas_to_torch(data_dict['train'][each_crop])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:32:31.396157Z","iopub.execute_input":"2025-12-18T21:32:31.396866Z","iopub.status.idle":"2025-12-18T21:32:34.198497Z","shell.execute_reply.started":"2025-12-18T21:32:31.396843Z","shell.execute_reply":"2025-12-18T21:32:34.197841Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"test_data.X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:32:41.983955Z","iopub.execute_input":"2025-12-18T21:32:41.984479Z","iopub.status.idle":"2025-12-18T21:32:41.989464Z","shell.execute_reply.started":"2025-12-18T21:32:41.984455Z","shell.execute_reply":"2025-12-18T21:32:41.988640Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"torch.Size([278747, 965])"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"data_dict['test'][each_crop]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T21:30:31.429070Z","iopub.execute_input":"2025-12-18T21:30:31.429364Z","iopub.status.idle":"2025-12-18T21:30:31.637726Z","shell.execute_reply.started":"2025-12-18T21:30:31.429345Z","shell.execute_reply":"2025-12-18T21:30:31.637001Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"          year     lon    lat      co2    nitrogen   tasmax_0   tasmax_1  \\\nID                                                                         \n1319737  420.0 -123.25  44.75   418.06  102.824997  31.621307  31.037140   \n1319738  420.0 -123.25  45.25   418.06  102.824997  29.953460  30.332184   \n1319739  420.0 -123.25  45.75   418.06  102.824997  26.864685  26.516113   \n1319740  420.0 -122.75  44.75   418.06  102.824997  31.807922  31.897370   \n1319741  420.0 -122.75  45.25   418.06  102.824997  31.803009  31.503723   \n...        ...     ...    ...      ...         ...        ...        ...   \n1873717  497.0  152.25 -29.25  1107.89   40.074001  23.115448  22.860565   \n1873718  497.0  152.25 -28.75  1107.89   40.074001  23.991821  23.926483   \n1873719  497.0  152.25 -28.25  1107.89   40.074001  24.289520  24.710602   \n1873720  497.0  152.25 -27.75  1107.89   40.074001  28.348694  29.666810   \n1873721  497.0  152.75 -28.75  1107.89   40.074001  26.626160  26.414154   \n\n          tasmax_2   tasmax_3   tasmax_4  ...   rsds_230   rsds_231  \\\nID                                        ...                         \n1319737  27.053986  27.319489  30.649078  ...  174.14244  150.82764   \n1319738  24.672333  26.461731  29.035553  ...  201.55307  153.14207   \n1319739  21.986877  23.700531  25.674530  ...  202.85130  158.17982   \n1319740  26.641174  27.302338  30.102540  ...  162.90735  153.26567   \n1319741  26.558655  28.035553  31.034210  ...  178.90230  155.62582   \n...            ...        ...        ...  ...        ...        ...   \n1873717  22.565948  23.216522  24.566772  ...  357.75784  236.90488   \n1873718  23.554413  24.288849  25.432434  ...  357.93820  250.80669   \n1873719  24.661255  25.235870  26.482025  ...  357.58700  267.46405   \n1873720  29.559906  29.040924  28.703827  ...  192.38712   73.39178   \n1873721  25.990906  26.945190  27.349152  ...  345.84320  278.55304   \n\n          rsds_232   rsds_233   rsds_234    rsds_235   rsds_236   rsds_237  \\\nID                                                                           \n1319737  190.36435  194.51694  100.82088  193.686020  242.53871  173.61038   \n1319738  190.76508  193.85138  107.75983  198.955750  227.22021  172.88612   \n1319739  185.61537  199.27208  100.38560  208.031400  212.48820  165.25601   \n1319740  192.76870  201.41548   99.03610  199.094280  248.47937  175.88953   \n1319741  188.06500  201.15889   94.03270  212.796590  232.24763  183.09230   \n...            ...        ...        ...         ...        ...        ...   \n1873717  146.83035  221.42387  157.71758   59.072372  250.26380  257.25482   \n1873718  174.62518  233.58775  169.67963   74.146860  250.39180  293.86765   \n1873719  187.97968  231.53940  172.55563   72.186110  245.47177  301.01040   \n1873720  185.05464  165.53415   81.64291  276.906770  284.31403  275.35962   \n1873721  177.29642  199.58263  153.73517   55.042465  217.63084  281.26315   \n\n          rsds_238    rsds_239  \nID                              \n1319737  158.46439  111.906140  \n1319738  167.09686  101.487076  \n1319739  158.21384  136.892970  \n1319740  170.87689   99.091990  \n1319741  176.92282  135.018230  \n...            ...         ...  \n1873717  290.07028  269.646880  \n1873718  284.58230  241.260910  \n1873719  272.35388  226.484740  \n1873720  283.39215  240.531940  \n1873721  263.87088  224.327760  \n\n[553878 rows x 965 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>co2</th>\n      <th>nitrogen</th>\n      <th>tasmax_0</th>\n      <th>tasmax_1</th>\n      <th>tasmax_2</th>\n      <th>tasmax_3</th>\n      <th>tasmax_4</th>\n      <th>...</th>\n      <th>rsds_230</th>\n      <th>rsds_231</th>\n      <th>rsds_232</th>\n      <th>rsds_233</th>\n      <th>rsds_234</th>\n      <th>rsds_235</th>\n      <th>rsds_236</th>\n      <th>rsds_237</th>\n      <th>rsds_238</th>\n      <th>rsds_239</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1319737</th>\n      <td>420.0</td>\n      <td>-123.25</td>\n      <td>44.75</td>\n      <td>418.06</td>\n      <td>102.824997</td>\n      <td>31.621307</td>\n      <td>31.037140</td>\n      <td>27.053986</td>\n      <td>27.319489</td>\n      <td>30.649078</td>\n      <td>...</td>\n      <td>174.14244</td>\n      <td>150.82764</td>\n      <td>190.36435</td>\n      <td>194.51694</td>\n      <td>100.82088</td>\n      <td>193.686020</td>\n      <td>242.53871</td>\n      <td>173.61038</td>\n      <td>158.46439</td>\n      <td>111.906140</td>\n    </tr>\n    <tr>\n      <th>1319738</th>\n      <td>420.0</td>\n      <td>-123.25</td>\n      <td>45.25</td>\n      <td>418.06</td>\n      <td>102.824997</td>\n      <td>29.953460</td>\n      <td>30.332184</td>\n      <td>24.672333</td>\n      <td>26.461731</td>\n      <td>29.035553</td>\n      <td>...</td>\n      <td>201.55307</td>\n      <td>153.14207</td>\n      <td>190.76508</td>\n      <td>193.85138</td>\n      <td>107.75983</td>\n      <td>198.955750</td>\n      <td>227.22021</td>\n      <td>172.88612</td>\n      <td>167.09686</td>\n      <td>101.487076</td>\n    </tr>\n    <tr>\n      <th>1319739</th>\n      <td>420.0</td>\n      <td>-123.25</td>\n      <td>45.75</td>\n      <td>418.06</td>\n      <td>102.824997</td>\n      <td>26.864685</td>\n      <td>26.516113</td>\n      <td>21.986877</td>\n      <td>23.700531</td>\n      <td>25.674530</td>\n      <td>...</td>\n      <td>202.85130</td>\n      <td>158.17982</td>\n      <td>185.61537</td>\n      <td>199.27208</td>\n      <td>100.38560</td>\n      <td>208.031400</td>\n      <td>212.48820</td>\n      <td>165.25601</td>\n      <td>158.21384</td>\n      <td>136.892970</td>\n    </tr>\n    <tr>\n      <th>1319740</th>\n      <td>420.0</td>\n      <td>-122.75</td>\n      <td>44.75</td>\n      <td>418.06</td>\n      <td>102.824997</td>\n      <td>31.807922</td>\n      <td>31.897370</td>\n      <td>26.641174</td>\n      <td>27.302338</td>\n      <td>30.102540</td>\n      <td>...</td>\n      <td>162.90735</td>\n      <td>153.26567</td>\n      <td>192.76870</td>\n      <td>201.41548</td>\n      <td>99.03610</td>\n      <td>199.094280</td>\n      <td>248.47937</td>\n      <td>175.88953</td>\n      <td>170.87689</td>\n      <td>99.091990</td>\n    </tr>\n    <tr>\n      <th>1319741</th>\n      <td>420.0</td>\n      <td>-122.75</td>\n      <td>45.25</td>\n      <td>418.06</td>\n      <td>102.824997</td>\n      <td>31.803009</td>\n      <td>31.503723</td>\n      <td>26.558655</td>\n      <td>28.035553</td>\n      <td>31.034210</td>\n      <td>...</td>\n      <td>178.90230</td>\n      <td>155.62582</td>\n      <td>188.06500</td>\n      <td>201.15889</td>\n      <td>94.03270</td>\n      <td>212.796590</td>\n      <td>232.24763</td>\n      <td>183.09230</td>\n      <td>176.92282</td>\n      <td>135.018230</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1873717</th>\n      <td>497.0</td>\n      <td>152.25</td>\n      <td>-29.25</td>\n      <td>1107.89</td>\n      <td>40.074001</td>\n      <td>23.115448</td>\n      <td>22.860565</td>\n      <td>22.565948</td>\n      <td>23.216522</td>\n      <td>24.566772</td>\n      <td>...</td>\n      <td>357.75784</td>\n      <td>236.90488</td>\n      <td>146.83035</td>\n      <td>221.42387</td>\n      <td>157.71758</td>\n      <td>59.072372</td>\n      <td>250.26380</td>\n      <td>257.25482</td>\n      <td>290.07028</td>\n      <td>269.646880</td>\n    </tr>\n    <tr>\n      <th>1873718</th>\n      <td>497.0</td>\n      <td>152.25</td>\n      <td>-28.75</td>\n      <td>1107.89</td>\n      <td>40.074001</td>\n      <td>23.991821</td>\n      <td>23.926483</td>\n      <td>23.554413</td>\n      <td>24.288849</td>\n      <td>25.432434</td>\n      <td>...</td>\n      <td>357.93820</td>\n      <td>250.80669</td>\n      <td>174.62518</td>\n      <td>233.58775</td>\n      <td>169.67963</td>\n      <td>74.146860</td>\n      <td>250.39180</td>\n      <td>293.86765</td>\n      <td>284.58230</td>\n      <td>241.260910</td>\n    </tr>\n    <tr>\n      <th>1873719</th>\n      <td>497.0</td>\n      <td>152.25</td>\n      <td>-28.25</td>\n      <td>1107.89</td>\n      <td>40.074001</td>\n      <td>24.289520</td>\n      <td>24.710602</td>\n      <td>24.661255</td>\n      <td>25.235870</td>\n      <td>26.482025</td>\n      <td>...</td>\n      <td>357.58700</td>\n      <td>267.46405</td>\n      <td>187.97968</td>\n      <td>231.53940</td>\n      <td>172.55563</td>\n      <td>72.186110</td>\n      <td>245.47177</td>\n      <td>301.01040</td>\n      <td>272.35388</td>\n      <td>226.484740</td>\n    </tr>\n    <tr>\n      <th>1873720</th>\n      <td>497.0</td>\n      <td>152.25</td>\n      <td>-27.75</td>\n      <td>1107.89</td>\n      <td>40.074001</td>\n      <td>28.348694</td>\n      <td>29.666810</td>\n      <td>29.559906</td>\n      <td>29.040924</td>\n      <td>28.703827</td>\n      <td>...</td>\n      <td>192.38712</td>\n      <td>73.39178</td>\n      <td>185.05464</td>\n      <td>165.53415</td>\n      <td>81.64291</td>\n      <td>276.906770</td>\n      <td>284.31403</td>\n      <td>275.35962</td>\n      <td>283.39215</td>\n      <td>240.531940</td>\n    </tr>\n    <tr>\n      <th>1873721</th>\n      <td>497.0</td>\n      <td>152.75</td>\n      <td>-28.75</td>\n      <td>1107.89</td>\n      <td>40.074001</td>\n      <td>26.626160</td>\n      <td>26.414154</td>\n      <td>25.990906</td>\n      <td>26.945190</td>\n      <td>27.349152</td>\n      <td>...</td>\n      <td>345.84320</td>\n      <td>278.55304</td>\n      <td>177.29642</td>\n      <td>199.58263</td>\n      <td>153.73517</td>\n      <td>55.042465</td>\n      <td>217.63084</td>\n      <td>281.26315</td>\n      <td>263.87088</td>\n      <td>224.327760</td>\n    </tr>\n  </tbody>\n</table>\n<p>553878 rows × 965 columns</p>\n</div>"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T16:58:10.183962Z","iopub.execute_input":"2025-12-16T16:58:10.184286Z","iopub.status.idle":"2025-12-16T16:58:10.221877Z","shell.execute_reply.started":"2025-12-16T16:58:10.184262Z","shell.execute_reply":"2025-12-16T16:58:10.221173Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"         year     lon    lat     co2    nitrogen   tasmax_0   tasmax_1  \\\nID                                                                       \n0       381.0 -122.25  48.25  340.79  186.110992  13.409790  12.907227   \n9047    382.0 -122.25  48.25  342.20  186.110992  16.964447  16.890717   \n17991   383.0 -122.25  48.25  343.78  186.110992  11.186615   9.690308   \n26895   384.0 -122.25  48.25  345.28  186.110992  13.291260  12.943298   \n35906   385.0 -122.25  48.25  346.80  186.110992   6.532745   6.931732   \n44909   386.0 -122.25  48.25  348.65  186.110992  11.285065   9.157837   \n53767   387.0 -122.25  48.25  350.74  186.110992  14.029297  14.028046   \n62770   388.0 -122.25  48.25  352.49  186.110992   9.260345   9.119843   \n71761   389.0 -122.25  48.25  353.86  186.110992   8.133850  11.753265   \n80796   390.0 -122.25  48.25  355.02  186.110992  12.798950  13.013062   \n89902   391.0 -122.25  48.25  355.89  186.110992  12.304077  13.935394   \n98845   392.0 -122.25  48.25  356.78  186.110992  11.580231  11.144073   \n107813  393.0 -122.25  48.25  358.13  186.110992  11.687805  16.592102   \n116891  394.0 -122.25  48.25  359.84  186.110992  15.268341  13.950531   \n125733  395.0 -122.25  48.25  361.46  186.110992  10.169861   8.479828   \n134738  396.0 -122.25  48.25  363.16  186.110992  11.886932  14.060120   \n143765  397.0 -122.25  48.25  365.32  186.110992  21.054626  20.052826   \n152649  398.0 -122.25  48.25  367.35  186.110992  11.199951  12.835876   \n161751  399.0 -122.25  48.25  368.87  186.110992  12.458099  13.669861   \n170536  400.0 -122.25  48.25  370.47  186.110992  13.823517  16.519684   \n179385  401.0 -122.25  48.25  372.52  186.110992  21.248260  21.101715   \n188365  402.0 -122.25  48.25  374.76  186.110992  15.310608  16.315308   \n197024  403.0 -122.25  48.25  376.81  186.110992   8.965485   9.519501   \n206117  404.0 -122.25  48.25  378.81  186.110992  13.841431  14.303162   \n215015  405.0 -122.25  48.25  380.93  186.110992  14.258545  17.882416   \n223925  406.0 -122.25  48.25  382.70  186.110992  15.817657  15.363831   \n232933  407.0 -122.25  48.25  384.77  186.110992  16.258972  15.658295   \n241831  408.0 -122.25  48.25  386.28  186.110992  10.649017  10.212799   \n250611  409.0 -122.25  48.25  388.57  186.110992  13.943176  14.445160   \n259462  410.0 -122.25  48.25  390.49  186.110992  10.730469  10.433929   \n268509  411.0 -122.25  48.25  392.52  186.110992  12.147583  12.134003   \n277523  412.0 -122.25  48.25  395.31  186.110992  11.882690  13.775055   \n286448  413.0 -122.25  48.25  397.12  186.110992  16.774567  16.157043   \n295391  414.0 -122.25  48.25  399.95  186.110992   9.702576  10.253113   \n304550  415.0 -122.25  48.25  403.12  186.110992  13.098206  13.199829   \n313583  416.0 -122.25  48.25  405.79  186.110992  11.686188  10.631500   \n322549  417.0 -122.25  48.25  408.76  186.110992  17.549835  15.187225   \n331593  418.0 -122.25  48.25  411.79  186.110992  12.915802   9.104584   \n340655  419.0 -122.25  48.25  414.89  186.110992  11.474243  15.426208   \n\n         tasmax_2   tasmax_3   tasmax_4  ...   rsds_230   rsds_231   rsds_232  \\\nID                                       ...                                    \n0       13.133698  21.765290  21.101470  ...  26.623010  24.254791  31.233927   \n9047    17.446442  10.807312  11.372833  ...  33.766327  14.601289   9.326722   \n17991   10.266022  13.817993  13.680969  ...  18.248117  24.222399  42.422520   \n26895   14.364929  15.389404  14.212402  ...  24.876170  20.465246  46.872010   \n35906    9.670837  13.058441  12.940735  ...  21.274920  22.775616  24.240604   \n44909   12.725403  14.007843  12.776703  ...  28.517231  40.694490  69.115530   \n53767   13.040009  11.844055   8.548218  ...  30.182318  27.349802  19.028444   \n62770   13.236603  11.942352  11.586243  ...  23.948435  57.682865  50.954594   \n71761   11.375000  11.365082  10.810120  ...  80.350470  89.696520  88.458954   \n80796   12.795990  13.321564  15.811218  ...  26.063366  35.390804  15.406098   \n89902   15.012970  17.456360  18.664246  ...  43.013830  40.481205  75.875330   \n98845   12.469879  14.462738  14.868042  ...  58.968864  68.643550  21.593271   \n107813  16.835571  15.537598  14.280518  ...   7.633269  24.939330  35.943604   \n116891  15.483887  14.507935  13.780640  ...  55.540268  14.414616  13.966405   \n125733  10.260559   9.311829   9.330841  ...  74.578255  78.687950  66.545160   \n134738  20.089447  21.964905  21.660004  ...  17.798826  34.138676  34.839200   \n143765  21.068176  20.786682  20.325745  ...  70.849556  29.107439  50.963860   \n152649  11.288452  10.044434  11.255157  ...  28.494345  36.673306  16.230461   \n161751  13.730713  12.107117  11.659149  ...  14.844245  32.627155  76.074260   \n170536  15.661652  15.596832  15.303253  ...  15.905437  16.343071  19.881160   \n179385  23.684875  22.931671  20.565887  ...  81.762230  28.693193  23.604134   \n188365  18.220673  20.865875  20.537231  ...  21.767717  21.523750  42.058823   \n197024   9.609283   8.629456  10.842895  ...  21.974330  28.904285  26.679344   \n206117  16.762482  18.912110  18.414307  ...  33.419613  19.442307  24.877487   \n215015  17.395325  16.265259  15.201050  ...  36.119926  18.547460   7.183670   \n223925  14.407989  12.704285  12.264343  ...  16.980556  26.870290  20.182709   \n232933  16.466827  18.086517  17.554870  ...  43.718388  32.989000  10.948646   \n241831   8.959381  10.477661   9.547913  ...  44.227430  74.479250  78.260506   \n250611  10.932556  11.233246  19.260376  ...   7.885307  31.402863  52.064472   \n259462   8.471222   7.521729  10.557617  ...  70.225560  63.357613  68.008545   \n268509  11.871002  15.590942  17.939545  ...  41.243630  56.955772  38.348000   \n277523  13.741852  12.491882   8.625061  ...  76.676410  52.497616  77.104454   \n286448  14.612030  18.771240  18.312317  ...  59.907240  51.216007  13.776854   \n295391  14.578003  12.237488  12.468689  ...  19.566027  33.463627  24.093050   \n304550  14.511169  13.546997  15.346344  ...  34.857944  40.702360  61.743404   \n313583  10.142914  10.422943  10.875275  ...  20.339586  25.147564  62.400043   \n322549  15.946808  10.908081  10.951904  ...   6.548294  45.260864  59.000730   \n331593  10.549042  13.652588  18.850922  ...  22.054132  15.829474  35.110720   \n340655  14.649139  15.198395  15.036591  ...  25.313625  42.187640  65.584090   \n\n         rsds_233   rsds_234   rsds_235   rsds_236   rsds_237   rsds_238  \\\nID                                                                         \n0       25.502360   6.170411  20.630627  47.028236  66.110880  13.543945   \n9047     7.116800   5.677502  43.132854  32.291283  33.324802  58.555800   \n17991   21.283678  11.998741  43.817760  22.971113   8.032106  14.891586   \n26895   76.881930  56.377876  30.735733  31.278101  30.236311  25.517010   \n35906    8.968406  38.302475  70.832860  73.302370   8.344541  14.387097   \n44909   71.985540  72.287990  73.706720  72.545456  68.653030  58.677704   \n53767   56.458310  72.359920  67.213170  14.359828  65.496796  17.437240   \n62770   78.830860  52.059170  11.518580  24.375566  34.588790  64.269750   \n71761   76.856610  10.951805  23.442772  10.872301  20.814053  17.406881   \n80796   31.865671  48.451645  59.080566  53.679123  57.162918  52.388596   \n89902   82.903076  78.340390  77.251010  78.581085  66.313385  74.806960   \n98845    5.653249  30.978685  48.607960   9.037930  34.810127  20.860062   \n107813  33.755264  35.479980  27.830776  17.881080  32.031948   6.342292   \n116891  15.553571  44.622395  66.226650  23.410206  18.496435  36.380016   \n125733  45.157852  61.982098  73.694890  74.431465  77.834030  75.476210   \n134738  41.933580  39.017838  40.261276  32.841236  40.461950   6.968429   \n143765  28.922358  69.090420  69.569900  71.606620  56.237724  81.313450   \n152649  28.563011  36.962387  66.999565  18.641336  30.010061  12.486845   \n161751  75.327070  73.631180  45.575676  57.558117  68.769485  67.519530   \n170536  16.503744  32.232136  30.663843  60.637108  37.672400  59.719948   \n179385  38.544370  12.824839  16.025219  27.625355  33.405220  61.822292   \n188365   7.968742  19.244616  30.418120  14.086078  40.506424  33.406208   \n197024   9.798622  40.218310  31.204525  35.847530  25.460617   8.853448   \n206117  20.656134  36.327187  39.756430  32.346210  32.113914  51.205944   \n215015  14.475078  43.832348  26.023998  17.909317  11.633003  24.129751   \n223925  49.445210  43.704945  49.902504  22.751215  37.545680  45.055637   \n232933   8.387229  51.747643  52.708523  12.738892  15.887134  72.823975   \n241831  46.608154  17.667376  19.832533  32.971290  25.065930   9.205993   \n250611  21.352360  23.774313  13.802475  38.705814  30.470198  28.930372   \n259462  67.912720  62.815315  66.985600  71.113430  77.233680  76.210240   \n268509  22.294043  57.584557  75.741920  79.891990  73.152440  72.680115   \n277523  72.687040  76.579956  83.337180  87.012530  81.751960  71.680820   \n286448  30.579744  36.679360  34.324978  11.115982   5.110580  34.877132   \n295391  35.997837  67.668670  84.029150  48.311460  19.921923  21.353207   \n304550  12.661536  23.846638  14.219810  39.765366  47.892310  49.317040   \n313583  20.827677  10.647055  38.755745  46.661983  47.147392  39.258835   \n322549  29.919062  19.279585  36.572740  18.598316  49.215527  56.792660   \n331593  54.106255  13.567299   8.989470  34.093124  15.917829   4.321158   \n340655  81.787820  36.391110  18.475492  28.471449  27.578106  17.351140   \n\n         rsds_239  \nID                 \n0       17.313820  \n9047    45.645490  \n17991   62.369316  \n26895   10.375072  \n35906   13.694097  \n44909   18.617320  \n53767   11.649876  \n62770   71.748920  \n71761   36.437310  \n80796   60.221340  \n89902   72.252174  \n98845   35.381805  \n107813  28.098656  \n116891  16.663675  \n125733  58.476960  \n134738  33.842870  \n143765  76.878624  \n152649  51.333680  \n161751  65.762650  \n170536  60.205246  \n179385  33.434150  \n188365  44.949825  \n197024  24.691935  \n206117  30.505276  \n215015  45.646830  \n223925  31.160826  \n232933  68.572334  \n241831  34.650604  \n250611   6.994615  \n259462  73.927740  \n268509  71.054990  \n277523  59.523300  \n286448  33.449770  \n295391  28.761870  \n304550  15.800364  \n313583  30.579311  \n322549  42.405310  \n331593  28.144947  \n340655  27.115833  \n\n[39 rows x 965 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>co2</th>\n      <th>nitrogen</th>\n      <th>tasmax_0</th>\n      <th>tasmax_1</th>\n      <th>tasmax_2</th>\n      <th>tasmax_3</th>\n      <th>tasmax_4</th>\n      <th>...</th>\n      <th>rsds_230</th>\n      <th>rsds_231</th>\n      <th>rsds_232</th>\n      <th>rsds_233</th>\n      <th>rsds_234</th>\n      <th>rsds_235</th>\n      <th>rsds_236</th>\n      <th>rsds_237</th>\n      <th>rsds_238</th>\n      <th>rsds_239</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>381.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>340.79</td>\n      <td>186.110992</td>\n      <td>13.409790</td>\n      <td>12.907227</td>\n      <td>13.133698</td>\n      <td>21.765290</td>\n      <td>21.101470</td>\n      <td>...</td>\n      <td>26.623010</td>\n      <td>24.254791</td>\n      <td>31.233927</td>\n      <td>25.502360</td>\n      <td>6.170411</td>\n      <td>20.630627</td>\n      <td>47.028236</td>\n      <td>66.110880</td>\n      <td>13.543945</td>\n      <td>17.313820</td>\n    </tr>\n    <tr>\n      <th>9047</th>\n      <td>382.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>342.20</td>\n      <td>186.110992</td>\n      <td>16.964447</td>\n      <td>16.890717</td>\n      <td>17.446442</td>\n      <td>10.807312</td>\n      <td>11.372833</td>\n      <td>...</td>\n      <td>33.766327</td>\n      <td>14.601289</td>\n      <td>9.326722</td>\n      <td>7.116800</td>\n      <td>5.677502</td>\n      <td>43.132854</td>\n      <td>32.291283</td>\n      <td>33.324802</td>\n      <td>58.555800</td>\n      <td>45.645490</td>\n    </tr>\n    <tr>\n      <th>17991</th>\n      <td>383.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>343.78</td>\n      <td>186.110992</td>\n      <td>11.186615</td>\n      <td>9.690308</td>\n      <td>10.266022</td>\n      <td>13.817993</td>\n      <td>13.680969</td>\n      <td>...</td>\n      <td>18.248117</td>\n      <td>24.222399</td>\n      <td>42.422520</td>\n      <td>21.283678</td>\n      <td>11.998741</td>\n      <td>43.817760</td>\n      <td>22.971113</td>\n      <td>8.032106</td>\n      <td>14.891586</td>\n      <td>62.369316</td>\n    </tr>\n    <tr>\n      <th>26895</th>\n      <td>384.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>345.28</td>\n      <td>186.110992</td>\n      <td>13.291260</td>\n      <td>12.943298</td>\n      <td>14.364929</td>\n      <td>15.389404</td>\n      <td>14.212402</td>\n      <td>...</td>\n      <td>24.876170</td>\n      <td>20.465246</td>\n      <td>46.872010</td>\n      <td>76.881930</td>\n      <td>56.377876</td>\n      <td>30.735733</td>\n      <td>31.278101</td>\n      <td>30.236311</td>\n      <td>25.517010</td>\n      <td>10.375072</td>\n    </tr>\n    <tr>\n      <th>35906</th>\n      <td>385.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>346.80</td>\n      <td>186.110992</td>\n      <td>6.532745</td>\n      <td>6.931732</td>\n      <td>9.670837</td>\n      <td>13.058441</td>\n      <td>12.940735</td>\n      <td>...</td>\n      <td>21.274920</td>\n      <td>22.775616</td>\n      <td>24.240604</td>\n      <td>8.968406</td>\n      <td>38.302475</td>\n      <td>70.832860</td>\n      <td>73.302370</td>\n      <td>8.344541</td>\n      <td>14.387097</td>\n      <td>13.694097</td>\n    </tr>\n    <tr>\n      <th>44909</th>\n      <td>386.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>348.65</td>\n      <td>186.110992</td>\n      <td>11.285065</td>\n      <td>9.157837</td>\n      <td>12.725403</td>\n      <td>14.007843</td>\n      <td>12.776703</td>\n      <td>...</td>\n      <td>28.517231</td>\n      <td>40.694490</td>\n      <td>69.115530</td>\n      <td>71.985540</td>\n      <td>72.287990</td>\n      <td>73.706720</td>\n      <td>72.545456</td>\n      <td>68.653030</td>\n      <td>58.677704</td>\n      <td>18.617320</td>\n    </tr>\n    <tr>\n      <th>53767</th>\n      <td>387.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>350.74</td>\n      <td>186.110992</td>\n      <td>14.029297</td>\n      <td>14.028046</td>\n      <td>13.040009</td>\n      <td>11.844055</td>\n      <td>8.548218</td>\n      <td>...</td>\n      <td>30.182318</td>\n      <td>27.349802</td>\n      <td>19.028444</td>\n      <td>56.458310</td>\n      <td>72.359920</td>\n      <td>67.213170</td>\n      <td>14.359828</td>\n      <td>65.496796</td>\n      <td>17.437240</td>\n      <td>11.649876</td>\n    </tr>\n    <tr>\n      <th>62770</th>\n      <td>388.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>352.49</td>\n      <td>186.110992</td>\n      <td>9.260345</td>\n      <td>9.119843</td>\n      <td>13.236603</td>\n      <td>11.942352</td>\n      <td>11.586243</td>\n      <td>...</td>\n      <td>23.948435</td>\n      <td>57.682865</td>\n      <td>50.954594</td>\n      <td>78.830860</td>\n      <td>52.059170</td>\n      <td>11.518580</td>\n      <td>24.375566</td>\n      <td>34.588790</td>\n      <td>64.269750</td>\n      <td>71.748920</td>\n    </tr>\n    <tr>\n      <th>71761</th>\n      <td>389.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>353.86</td>\n      <td>186.110992</td>\n      <td>8.133850</td>\n      <td>11.753265</td>\n      <td>11.375000</td>\n      <td>11.365082</td>\n      <td>10.810120</td>\n      <td>...</td>\n      <td>80.350470</td>\n      <td>89.696520</td>\n      <td>88.458954</td>\n      <td>76.856610</td>\n      <td>10.951805</td>\n      <td>23.442772</td>\n      <td>10.872301</td>\n      <td>20.814053</td>\n      <td>17.406881</td>\n      <td>36.437310</td>\n    </tr>\n    <tr>\n      <th>80796</th>\n      <td>390.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>355.02</td>\n      <td>186.110992</td>\n      <td>12.798950</td>\n      <td>13.013062</td>\n      <td>12.795990</td>\n      <td>13.321564</td>\n      <td>15.811218</td>\n      <td>...</td>\n      <td>26.063366</td>\n      <td>35.390804</td>\n      <td>15.406098</td>\n      <td>31.865671</td>\n      <td>48.451645</td>\n      <td>59.080566</td>\n      <td>53.679123</td>\n      <td>57.162918</td>\n      <td>52.388596</td>\n      <td>60.221340</td>\n    </tr>\n    <tr>\n      <th>89902</th>\n      <td>391.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>355.89</td>\n      <td>186.110992</td>\n      <td>12.304077</td>\n      <td>13.935394</td>\n      <td>15.012970</td>\n      <td>17.456360</td>\n      <td>18.664246</td>\n      <td>...</td>\n      <td>43.013830</td>\n      <td>40.481205</td>\n      <td>75.875330</td>\n      <td>82.903076</td>\n      <td>78.340390</td>\n      <td>77.251010</td>\n      <td>78.581085</td>\n      <td>66.313385</td>\n      <td>74.806960</td>\n      <td>72.252174</td>\n    </tr>\n    <tr>\n      <th>98845</th>\n      <td>392.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>356.78</td>\n      <td>186.110992</td>\n      <td>11.580231</td>\n      <td>11.144073</td>\n      <td>12.469879</td>\n      <td>14.462738</td>\n      <td>14.868042</td>\n      <td>...</td>\n      <td>58.968864</td>\n      <td>68.643550</td>\n      <td>21.593271</td>\n      <td>5.653249</td>\n      <td>30.978685</td>\n      <td>48.607960</td>\n      <td>9.037930</td>\n      <td>34.810127</td>\n      <td>20.860062</td>\n      <td>35.381805</td>\n    </tr>\n    <tr>\n      <th>107813</th>\n      <td>393.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>358.13</td>\n      <td>186.110992</td>\n      <td>11.687805</td>\n      <td>16.592102</td>\n      <td>16.835571</td>\n      <td>15.537598</td>\n      <td>14.280518</td>\n      <td>...</td>\n      <td>7.633269</td>\n      <td>24.939330</td>\n      <td>35.943604</td>\n      <td>33.755264</td>\n      <td>35.479980</td>\n      <td>27.830776</td>\n      <td>17.881080</td>\n      <td>32.031948</td>\n      <td>6.342292</td>\n      <td>28.098656</td>\n    </tr>\n    <tr>\n      <th>116891</th>\n      <td>394.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>359.84</td>\n      <td>186.110992</td>\n      <td>15.268341</td>\n      <td>13.950531</td>\n      <td>15.483887</td>\n      <td>14.507935</td>\n      <td>13.780640</td>\n      <td>...</td>\n      <td>55.540268</td>\n      <td>14.414616</td>\n      <td>13.966405</td>\n      <td>15.553571</td>\n      <td>44.622395</td>\n      <td>66.226650</td>\n      <td>23.410206</td>\n      <td>18.496435</td>\n      <td>36.380016</td>\n      <td>16.663675</td>\n    </tr>\n    <tr>\n      <th>125733</th>\n      <td>395.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>361.46</td>\n      <td>186.110992</td>\n      <td>10.169861</td>\n      <td>8.479828</td>\n      <td>10.260559</td>\n      <td>9.311829</td>\n      <td>9.330841</td>\n      <td>...</td>\n      <td>74.578255</td>\n      <td>78.687950</td>\n      <td>66.545160</td>\n      <td>45.157852</td>\n      <td>61.982098</td>\n      <td>73.694890</td>\n      <td>74.431465</td>\n      <td>77.834030</td>\n      <td>75.476210</td>\n      <td>58.476960</td>\n    </tr>\n    <tr>\n      <th>134738</th>\n      <td>396.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>363.16</td>\n      <td>186.110992</td>\n      <td>11.886932</td>\n      <td>14.060120</td>\n      <td>20.089447</td>\n      <td>21.964905</td>\n      <td>21.660004</td>\n      <td>...</td>\n      <td>17.798826</td>\n      <td>34.138676</td>\n      <td>34.839200</td>\n      <td>41.933580</td>\n      <td>39.017838</td>\n      <td>40.261276</td>\n      <td>32.841236</td>\n      <td>40.461950</td>\n      <td>6.968429</td>\n      <td>33.842870</td>\n    </tr>\n    <tr>\n      <th>143765</th>\n      <td>397.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>365.32</td>\n      <td>186.110992</td>\n      <td>21.054626</td>\n      <td>20.052826</td>\n      <td>21.068176</td>\n      <td>20.786682</td>\n      <td>20.325745</td>\n      <td>...</td>\n      <td>70.849556</td>\n      <td>29.107439</td>\n      <td>50.963860</td>\n      <td>28.922358</td>\n      <td>69.090420</td>\n      <td>69.569900</td>\n      <td>71.606620</td>\n      <td>56.237724</td>\n      <td>81.313450</td>\n      <td>76.878624</td>\n    </tr>\n    <tr>\n      <th>152649</th>\n      <td>398.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>367.35</td>\n      <td>186.110992</td>\n      <td>11.199951</td>\n      <td>12.835876</td>\n      <td>11.288452</td>\n      <td>10.044434</td>\n      <td>11.255157</td>\n      <td>...</td>\n      <td>28.494345</td>\n      <td>36.673306</td>\n      <td>16.230461</td>\n      <td>28.563011</td>\n      <td>36.962387</td>\n      <td>66.999565</td>\n      <td>18.641336</td>\n      <td>30.010061</td>\n      <td>12.486845</td>\n      <td>51.333680</td>\n    </tr>\n    <tr>\n      <th>161751</th>\n      <td>399.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>368.87</td>\n      <td>186.110992</td>\n      <td>12.458099</td>\n      <td>13.669861</td>\n      <td>13.730713</td>\n      <td>12.107117</td>\n      <td>11.659149</td>\n      <td>...</td>\n      <td>14.844245</td>\n      <td>32.627155</td>\n      <td>76.074260</td>\n      <td>75.327070</td>\n      <td>73.631180</td>\n      <td>45.575676</td>\n      <td>57.558117</td>\n      <td>68.769485</td>\n      <td>67.519530</td>\n      <td>65.762650</td>\n    </tr>\n    <tr>\n      <th>170536</th>\n      <td>400.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>370.47</td>\n      <td>186.110992</td>\n      <td>13.823517</td>\n      <td>16.519684</td>\n      <td>15.661652</td>\n      <td>15.596832</td>\n      <td>15.303253</td>\n      <td>...</td>\n      <td>15.905437</td>\n      <td>16.343071</td>\n      <td>19.881160</td>\n      <td>16.503744</td>\n      <td>32.232136</td>\n      <td>30.663843</td>\n      <td>60.637108</td>\n      <td>37.672400</td>\n      <td>59.719948</td>\n      <td>60.205246</td>\n    </tr>\n    <tr>\n      <th>179385</th>\n      <td>401.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>372.52</td>\n      <td>186.110992</td>\n      <td>21.248260</td>\n      <td>21.101715</td>\n      <td>23.684875</td>\n      <td>22.931671</td>\n      <td>20.565887</td>\n      <td>...</td>\n      <td>81.762230</td>\n      <td>28.693193</td>\n      <td>23.604134</td>\n      <td>38.544370</td>\n      <td>12.824839</td>\n      <td>16.025219</td>\n      <td>27.625355</td>\n      <td>33.405220</td>\n      <td>61.822292</td>\n      <td>33.434150</td>\n    </tr>\n    <tr>\n      <th>188365</th>\n      <td>402.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>374.76</td>\n      <td>186.110992</td>\n      <td>15.310608</td>\n      <td>16.315308</td>\n      <td>18.220673</td>\n      <td>20.865875</td>\n      <td>20.537231</td>\n      <td>...</td>\n      <td>21.767717</td>\n      <td>21.523750</td>\n      <td>42.058823</td>\n      <td>7.968742</td>\n      <td>19.244616</td>\n      <td>30.418120</td>\n      <td>14.086078</td>\n      <td>40.506424</td>\n      <td>33.406208</td>\n      <td>44.949825</td>\n    </tr>\n    <tr>\n      <th>197024</th>\n      <td>403.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>376.81</td>\n      <td>186.110992</td>\n      <td>8.965485</td>\n      <td>9.519501</td>\n      <td>9.609283</td>\n      <td>8.629456</td>\n      <td>10.842895</td>\n      <td>...</td>\n      <td>21.974330</td>\n      <td>28.904285</td>\n      <td>26.679344</td>\n      <td>9.798622</td>\n      <td>40.218310</td>\n      <td>31.204525</td>\n      <td>35.847530</td>\n      <td>25.460617</td>\n      <td>8.853448</td>\n      <td>24.691935</td>\n    </tr>\n    <tr>\n      <th>206117</th>\n      <td>404.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>378.81</td>\n      <td>186.110992</td>\n      <td>13.841431</td>\n      <td>14.303162</td>\n      <td>16.762482</td>\n      <td>18.912110</td>\n      <td>18.414307</td>\n      <td>...</td>\n      <td>33.419613</td>\n      <td>19.442307</td>\n      <td>24.877487</td>\n      <td>20.656134</td>\n      <td>36.327187</td>\n      <td>39.756430</td>\n      <td>32.346210</td>\n      <td>32.113914</td>\n      <td>51.205944</td>\n      <td>30.505276</td>\n    </tr>\n    <tr>\n      <th>215015</th>\n      <td>405.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>380.93</td>\n      <td>186.110992</td>\n      <td>14.258545</td>\n      <td>17.882416</td>\n      <td>17.395325</td>\n      <td>16.265259</td>\n      <td>15.201050</td>\n      <td>...</td>\n      <td>36.119926</td>\n      <td>18.547460</td>\n      <td>7.183670</td>\n      <td>14.475078</td>\n      <td>43.832348</td>\n      <td>26.023998</td>\n      <td>17.909317</td>\n      <td>11.633003</td>\n      <td>24.129751</td>\n      <td>45.646830</td>\n    </tr>\n    <tr>\n      <th>223925</th>\n      <td>406.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>382.70</td>\n      <td>186.110992</td>\n      <td>15.817657</td>\n      <td>15.363831</td>\n      <td>14.407989</td>\n      <td>12.704285</td>\n      <td>12.264343</td>\n      <td>...</td>\n      <td>16.980556</td>\n      <td>26.870290</td>\n      <td>20.182709</td>\n      <td>49.445210</td>\n      <td>43.704945</td>\n      <td>49.902504</td>\n      <td>22.751215</td>\n      <td>37.545680</td>\n      <td>45.055637</td>\n      <td>31.160826</td>\n    </tr>\n    <tr>\n      <th>232933</th>\n      <td>407.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>384.77</td>\n      <td>186.110992</td>\n      <td>16.258972</td>\n      <td>15.658295</td>\n      <td>16.466827</td>\n      <td>18.086517</td>\n      <td>17.554870</td>\n      <td>...</td>\n      <td>43.718388</td>\n      <td>32.989000</td>\n      <td>10.948646</td>\n      <td>8.387229</td>\n      <td>51.747643</td>\n      <td>52.708523</td>\n      <td>12.738892</td>\n      <td>15.887134</td>\n      <td>72.823975</td>\n      <td>68.572334</td>\n    </tr>\n    <tr>\n      <th>241831</th>\n      <td>408.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>386.28</td>\n      <td>186.110992</td>\n      <td>10.649017</td>\n      <td>10.212799</td>\n      <td>8.959381</td>\n      <td>10.477661</td>\n      <td>9.547913</td>\n      <td>...</td>\n      <td>44.227430</td>\n      <td>74.479250</td>\n      <td>78.260506</td>\n      <td>46.608154</td>\n      <td>17.667376</td>\n      <td>19.832533</td>\n      <td>32.971290</td>\n      <td>25.065930</td>\n      <td>9.205993</td>\n      <td>34.650604</td>\n    </tr>\n    <tr>\n      <th>250611</th>\n      <td>409.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>388.57</td>\n      <td>186.110992</td>\n      <td>13.943176</td>\n      <td>14.445160</td>\n      <td>10.932556</td>\n      <td>11.233246</td>\n      <td>19.260376</td>\n      <td>...</td>\n      <td>7.885307</td>\n      <td>31.402863</td>\n      <td>52.064472</td>\n      <td>21.352360</td>\n      <td>23.774313</td>\n      <td>13.802475</td>\n      <td>38.705814</td>\n      <td>30.470198</td>\n      <td>28.930372</td>\n      <td>6.994615</td>\n    </tr>\n    <tr>\n      <th>259462</th>\n      <td>410.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>390.49</td>\n      <td>186.110992</td>\n      <td>10.730469</td>\n      <td>10.433929</td>\n      <td>8.471222</td>\n      <td>7.521729</td>\n      <td>10.557617</td>\n      <td>...</td>\n      <td>70.225560</td>\n      <td>63.357613</td>\n      <td>68.008545</td>\n      <td>67.912720</td>\n      <td>62.815315</td>\n      <td>66.985600</td>\n      <td>71.113430</td>\n      <td>77.233680</td>\n      <td>76.210240</td>\n      <td>73.927740</td>\n    </tr>\n    <tr>\n      <th>268509</th>\n      <td>411.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>392.52</td>\n      <td>186.110992</td>\n      <td>12.147583</td>\n      <td>12.134003</td>\n      <td>11.871002</td>\n      <td>15.590942</td>\n      <td>17.939545</td>\n      <td>...</td>\n      <td>41.243630</td>\n      <td>56.955772</td>\n      <td>38.348000</td>\n      <td>22.294043</td>\n      <td>57.584557</td>\n      <td>75.741920</td>\n      <td>79.891990</td>\n      <td>73.152440</td>\n      <td>72.680115</td>\n      <td>71.054990</td>\n    </tr>\n    <tr>\n      <th>277523</th>\n      <td>412.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>395.31</td>\n      <td>186.110992</td>\n      <td>11.882690</td>\n      <td>13.775055</td>\n      <td>13.741852</td>\n      <td>12.491882</td>\n      <td>8.625061</td>\n      <td>...</td>\n      <td>76.676410</td>\n      <td>52.497616</td>\n      <td>77.104454</td>\n      <td>72.687040</td>\n      <td>76.579956</td>\n      <td>83.337180</td>\n      <td>87.012530</td>\n      <td>81.751960</td>\n      <td>71.680820</td>\n      <td>59.523300</td>\n    </tr>\n    <tr>\n      <th>286448</th>\n      <td>413.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>397.12</td>\n      <td>186.110992</td>\n      <td>16.774567</td>\n      <td>16.157043</td>\n      <td>14.612030</td>\n      <td>18.771240</td>\n      <td>18.312317</td>\n      <td>...</td>\n      <td>59.907240</td>\n      <td>51.216007</td>\n      <td>13.776854</td>\n      <td>30.579744</td>\n      <td>36.679360</td>\n      <td>34.324978</td>\n      <td>11.115982</td>\n      <td>5.110580</td>\n      <td>34.877132</td>\n      <td>33.449770</td>\n    </tr>\n    <tr>\n      <th>295391</th>\n      <td>414.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>399.95</td>\n      <td>186.110992</td>\n      <td>9.702576</td>\n      <td>10.253113</td>\n      <td>14.578003</td>\n      <td>12.237488</td>\n      <td>12.468689</td>\n      <td>...</td>\n      <td>19.566027</td>\n      <td>33.463627</td>\n      <td>24.093050</td>\n      <td>35.997837</td>\n      <td>67.668670</td>\n      <td>84.029150</td>\n      <td>48.311460</td>\n      <td>19.921923</td>\n      <td>21.353207</td>\n      <td>28.761870</td>\n    </tr>\n    <tr>\n      <th>304550</th>\n      <td>415.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>403.12</td>\n      <td>186.110992</td>\n      <td>13.098206</td>\n      <td>13.199829</td>\n      <td>14.511169</td>\n      <td>13.546997</td>\n      <td>15.346344</td>\n      <td>...</td>\n      <td>34.857944</td>\n      <td>40.702360</td>\n      <td>61.743404</td>\n      <td>12.661536</td>\n      <td>23.846638</td>\n      <td>14.219810</td>\n      <td>39.765366</td>\n      <td>47.892310</td>\n      <td>49.317040</td>\n      <td>15.800364</td>\n    </tr>\n    <tr>\n      <th>313583</th>\n      <td>416.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>405.79</td>\n      <td>186.110992</td>\n      <td>11.686188</td>\n      <td>10.631500</td>\n      <td>10.142914</td>\n      <td>10.422943</td>\n      <td>10.875275</td>\n      <td>...</td>\n      <td>20.339586</td>\n      <td>25.147564</td>\n      <td>62.400043</td>\n      <td>20.827677</td>\n      <td>10.647055</td>\n      <td>38.755745</td>\n      <td>46.661983</td>\n      <td>47.147392</td>\n      <td>39.258835</td>\n      <td>30.579311</td>\n    </tr>\n    <tr>\n      <th>322549</th>\n      <td>417.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>408.76</td>\n      <td>186.110992</td>\n      <td>17.549835</td>\n      <td>15.187225</td>\n      <td>15.946808</td>\n      <td>10.908081</td>\n      <td>10.951904</td>\n      <td>...</td>\n      <td>6.548294</td>\n      <td>45.260864</td>\n      <td>59.000730</td>\n      <td>29.919062</td>\n      <td>19.279585</td>\n      <td>36.572740</td>\n      <td>18.598316</td>\n      <td>49.215527</td>\n      <td>56.792660</td>\n      <td>42.405310</td>\n    </tr>\n    <tr>\n      <th>331593</th>\n      <td>418.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>411.79</td>\n      <td>186.110992</td>\n      <td>12.915802</td>\n      <td>9.104584</td>\n      <td>10.549042</td>\n      <td>13.652588</td>\n      <td>18.850922</td>\n      <td>...</td>\n      <td>22.054132</td>\n      <td>15.829474</td>\n      <td>35.110720</td>\n      <td>54.106255</td>\n      <td>13.567299</td>\n      <td>8.989470</td>\n      <td>34.093124</td>\n      <td>15.917829</td>\n      <td>4.321158</td>\n      <td>28.144947</td>\n    </tr>\n    <tr>\n      <th>340655</th>\n      <td>419.0</td>\n      <td>-122.25</td>\n      <td>48.25</td>\n      <td>414.89</td>\n      <td>186.110992</td>\n      <td>11.474243</td>\n      <td>15.426208</td>\n      <td>14.649139</td>\n      <td>15.198395</td>\n      <td>15.036591</td>\n      <td>...</td>\n      <td>25.313625</td>\n      <td>42.187640</td>\n      <td>65.584090</td>\n      <td>81.787820</td>\n      <td>36.391110</td>\n      <td>18.475492</td>\n      <td>28.471449</td>\n      <td>27.578106</td>\n      <td>17.351140</td>\n      <td>27.115833</td>\n    </tr>\n  </tbody>\n</table>\n<p>39 rows × 965 columns</p>\n</div>"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"##  train_locs.groupby(['lon','lat']).groupsDoes our vector-MLP work across regions?\n\nMy expectation is no, but let's give it a go.\n\nTesting fit the expectation. Very hard to get cross-region validation loss below 1.5 .","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n\n## We need to redefine the dataset, but otherwise the model architecture is the same (with slight input size difference)\nDATA_DIR = r'/kaggle/input/the-future-crop-challenge'\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nimport gc\n\nclass VectorisedData(Dataset):\n    \"\"\"\n    A pytorch dataset class that transfers all data to GPU, minimising memory transfers during training.\n    Expands static data () to the length of the sequence.\n    \"\"\"\n    def __init__(self, crop: str, mode: str, data_dir: str, device=DEVICE):\n        # Read all data files\n        self.prepare_data(crop, mode, data_dir, device)\n    \n    def prepare_data(self, crop,mode,data_dir,device):\n        tasmax = pd.read_parquet(os.path.join(data_dir, f\"tasmax_{crop}_{mode}.parquet\"))\n        tasmin = pd.read_parquet(os.path.join(data_dir, f\"tasmin_{crop}_{mode}.parquet\"))\n        pr = pd.read_parquet(os.path.join(data_dir, f\"pr_{crop}_{mode}.parquet\"))\n        rsds = pd.read_parquet(os.path.join(data_dir, f\"rsds_{crop}_{mode}.parquet\"))\n        soil_co2 = pd.read_parquet(os.path.join(data_dir, f\"soil_co2_{crop}_{mode}.parquet\"))\n        \n        # Load yield data if in training mode\n        if mode == 'train':\n            self.yield_data = pd.read_parquet(os.path.join(data_dir, f\"{mode}_solutions_{crop}.parquet\"))\n            self.yield_data['crop'] = crop\n        else:\n            self.yield_data = pd.DataFrame(tasmax['crop'])\n            self.yield_data['yield'] = 0\n            \n        # Preprocess climate data in bulk\n        climate_data = np.concatenate([\n            tasmax.iloc[:, 5:].values,\n            tasmin.iloc[:, 5:].values,\n            pr.iloc[:, 5:].values,\n            rsds.iloc[:, 5:].values\n        ], axis=1).astype(np.float32) #(n_samples,240x4)\n\n        \n        # Preprocess soil data in bulk\n        soil_continuous = soil_co2[['lon', 'lat', 'co2']].values.astype(np.float32) #(n_samples, 3)\n        yield_expanded = self.yield_data['yield'].values.reshape(-1,1) #(n_samples, 1)\n    \n        # Combine climate and soil features (shape: num_samples × 240 × 21)\n        full_input = np.concatenate([climate_data, soil_continuous], axis=1) #(n_samples, 240x4+3)\n        \n        # Move entire dataset to device in one operation\n        self.inputs = torch.tensor(full_input, device=device,dtype=torch.float32)\n        self.targets = torch.tensor(yield_expanded,device=device,dtype=torch.float32)\n        #memory management\n        del tasmax, tasmin, pr, rsds, soil_co2, climate_data, soil_continuous, yield_expanded, full_input\n\n        gc.collect()\n        \n        return None\n        \n    def __getitem__(self, index):\n        # Return precomputed tensors\n        return self.inputs[index], self.targets[index]\n\n    def __len__(self):\n        return len(self.inputs)\n\nprint('Loading training data.,. (this may take a while)')\n\ntrain_maize = VectorisedData('maize','train',DATA_DIR)\n#train_wheat = VectorisedData('wheat','train', DATA_DIR)\n\nprint(train_maize.inputs.shape)\n\nprint('Finished loading')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T19:33:41.406845Z","iopub.execute_input":"2025-12-04T19:33:41.407630Z","iopub.status.idle":"2025-12-04T19:34:03.360898Z","shell.execute_reply.started":"2025-12-04T19:33:41.407595Z","shell.execute_reply":"2025-12-04T19:34:03.360109Z"}},"outputs":[{"name":"stdout","text":"Loading training data.,. (this may take a while)\ntorch.Size([349719, 963])\nFinished loading\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"## just pasting the training loop from above:\n\n\n# --- 3. Flexible Model Definition and Xavier Initialization (DNN) ---\n\nclass FlexibleFlattenedDNN(nn.Module):\n    def __init__(self, input_size, output_size, num_layers, reduction_ratio, dropout_rate=0.0, leaky_relu_slope =0.01):\n        super(FlexibleFlattenedDNN, self).__init__()\n        self.leaky_slope = leaky_relu_slope\n        layers = []\n        current_size = input_size\n        \n        # Dynamically build the hidden layers\n        for i in range(num_layers):\n            # Calculate the size of the next layer\n            next_size = max(4, int(current_size * reduction_ratio)) # Min size of 4 for stability\n            \n            # Add Linear Layer\n            layers.append(nn.Linear(current_size, next_size))\n            # Add Activation\n            layers.append(nn.LeakyReLU(self.leaky_slope))\n            \n            # Add Dropout (only for intermediate layers)\n            if dropout_rate > 0 and i < num_layers - 1:\n                 layers.append(nn.Dropout(dropout_rate)) \n                \n            current_size = next_size\n        \n        # Add the final output layer (no activation or dropout after this)\n        layers.append(nn.Linear(current_size, output_size))\n        \n        self.fc_stack = nn.Sequential(*layers)\n        \n        print(f\"DNN Architecture built: {input_size} -> {[l.out_features for l in layers if isinstance(l, nn.Linear)]}\")\n        \n        self._init_weights()\n\n    def _init_weights(self):\n        # Xavier/Glorot Initialization for all Linear layers\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                #nn.init.xavier_uniform_(m.weight)\n                nn.init.kaiming_normal_(m.weight, a= self.leaky_slope)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        print(\"Model weights initialized with Xavier/Glorot.\")\n\n    def forward(self, x):\n        return self.fc_stack(x) \n\n\n# --- MODEL FLEXIBILITY PARAMETERS ---\nNUM_HIDDEN_LAYERS = 8    # The number of layers between input and output (e.g., 3 for 4 layers total)\nREDUCTION_RATIO = 1/2      # The ratio by which each layer size decreases (e.g., 0.5 means half the size)\nDROPOUT_RATE = 2/5        # Dropout rate for intermediate layers (0.0 for no dropout)\n\n# --- CALCULATED DIMENSIONS ---\nFLATTENED_INPUT_SIZE = 963\nTARGET_OUTPUT_SIZE = 1 # Corrected to 1D output\n\n# Hyperparameter\ninit_LR = 1e-4\nmax_LR = 1e-3\nweight_decay = 1e-5 #suggested to be smaller for 'super-convergence' in OneCycleLR paper.\nn_epochs = 1000\nn_batch = 512\n\n\n# Create DataLoader\nn_train = int(train_maize.inputs.shape[0]*0.8)\nn_test = train_maize.inputs.shape[0]-n_train\n\ntrain_dataset = torch.utils.data.Subset(train_maize, range(n_train))\nval_dataset = torch.utils.data.Subset(train_maize,range(n_train,n_train+n_test))\n\ntrain_loader = DataLoader(train_dataset, batch_size=n_batch, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size = n_batch, shuffle = False)\n\n# Instantiate the flexible model\nmodel = FlexibleFlattenedDNN(\n    input_size=FLATTENED_INPUT_SIZE,\n    output_size=TARGET_OUTPUT_SIZE,\n    num_layers=NUM_HIDDEN_LAYERS,\n    reduction_ratio=REDUCTION_RATIO,\n    dropout_rate=DROPOUT_RATE\n).to(DEVICE, dtype=torch.float32)\n\nn_params = sum([p.numel() for p in model.parameters()])\nprint(f\"Model instantiatied with {n_params} parameters.\")\n# --- 4. Loss and Optimizer ---\n\n# Switched back to MSELoss as requested in the original code block\ncriterion_primary = nn.MSELoss().to(DEVICE) \noptimizer = optim.AdamW(model.parameters(), lr=init_LR, weight_decay = weight_decay)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_LR, \n                                          steps_per_epoch=len(train_loader), \n                                          epochs=n_epochs)\n\n# --- 5. Training Loop (Unchanged) ---\n\nprint(\"-\" * 30)\nprint(f\"Starting Training for {n_epochs} epochs...\")\nmodel.train() \n\nfor epoch in range(1, n_epochs + 1):\n    for inputs_batch, targets_batch in train_loader:\n        # Backward and optimize\n        optimizer.zero_grad()\n        \n        # Forward pass:\n        outputs = model(inputs_batch)\n        \n        # Calculate loss (outputs: [N, 1], targets: [N, 1])\n        loss = criterion_primary(outputs, targets_batch)\n        \n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    if epoch % 10 == 0 or epoch == 1:\n        avg_loss = loss \n        print(f'Epoch [{epoch:05d}/{n_epochs}], Loss: {avg_loss:.6f}')\n        model.eval()\n        with torch.no_grad():\n            val_losses = []\n            for val_inputs, val_targets in val_loader:\n                val_preds = model(val_inputs)\n                val_losses.append(criterion_primary(val_preds,val_targets).item())\n\n        print(f'Validation loss: {np.mean(val_losses):.6f}')\nprint(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T20:13:19.253367Z","iopub.execute_input":"2025-12-04T20:13:19.254021Z"}},"outputs":[{"name":"stdout","text":"DNN Architecture built: 963 -> [481, 240, 120, 60, 30, 15, 7, 4, 1]\nModel weights initialized with Xavier/Glorot.\nModel instantiatied with 617988 parameters.\n------------------------------\nStarting Training for 1000 epochs...\nEpoch [00001/1000], Loss: 1552.913696\nValidation loss: 18.472941\nEpoch [00010/1000], Loss: 6.316211\nValidation loss: 5.749801\nEpoch [00020/1000], Loss: 4.706806\nValidation loss: 3.751177\nEpoch [00030/1000], Loss: 3.517475\nValidation loss: 3.020003\nEpoch [00040/1000], Loss: 2.530788\nValidation loss: 2.806074\nEpoch [00050/1000], Loss: 1.581303\nValidation loss: 2.971395\nEpoch [00060/1000], Loss: 0.988292\nValidation loss: 2.739290\nEpoch [00070/1000], Loss: 1.183306\nValidation loss: 2.796676\nEpoch [00080/1000], Loss: 1.374236\nValidation loss: 2.763794\nEpoch [00090/1000], Loss: 1.179546\nValidation loss: 2.700817\nEpoch [00100/1000], Loss: 1.116433\nValidation loss: 2.594989\nEpoch [00110/1000], Loss: 0.805130\nValidation loss: 2.718849\nEpoch [00120/1000], Loss: 0.829813\nValidation loss: 2.647773\nEpoch [00130/1000], Loss: 0.902059\nValidation loss: 2.537796\nEpoch [00140/1000], Loss: 0.805387\nValidation loss: 2.965111\nEpoch [00150/1000], Loss: 0.998089\nValidation loss: 2.636956\nEpoch [00160/1000], Loss: 0.865719\nValidation loss: 2.577779\nEpoch [00170/1000], Loss: 0.934451\nValidation loss: 2.849511\nEpoch [00180/1000], Loss: 0.761250\nValidation loss: 2.654899\nEpoch [00190/1000], Loss: 1.333006\nValidation loss: 2.559126\nEpoch [00200/1000], Loss: 0.790862\nValidation loss: 2.585845\nEpoch [00210/1000], Loss: 0.889345\nValidation loss: 2.530566\nEpoch [00220/1000], Loss: 0.774608\nValidation loss: 2.519514\nEpoch [00230/1000], Loss: 0.768407\nValidation loss: 2.500410\nEpoch [00240/1000], Loss: 1.006499\nValidation loss: 2.597036\nEpoch [00250/1000], Loss: 0.752841\nValidation loss: 2.811652\nEpoch [00260/1000], Loss: 0.805115\nValidation loss: 2.649880\nEpoch [00270/1000], Loss: 0.706095\nValidation loss: 2.596046\nEpoch [00280/1000], Loss: 0.599436\nValidation loss: 2.521851\nEpoch [00290/1000], Loss: 0.809091\nValidation loss: 2.681918\nEpoch [00300/1000], Loss: 0.629206\nValidation loss: 2.534880\nEpoch [00310/1000], Loss: 0.785903\nValidation loss: 2.839071\nEpoch [00320/1000], Loss: 0.654251\nValidation loss: 2.766321\nEpoch [00330/1000], Loss: 0.631223\nValidation loss: 2.674430\nEpoch [00340/1000], Loss: 0.709219\nValidation loss: 2.765287\nEpoch [00350/1000], Loss: 0.571406\nValidation loss: 2.671358\nEpoch [00360/1000], Loss: 0.565766\nValidation loss: 2.715094\nEpoch [00370/1000], Loss: 0.529381\nValidation loss: 2.729247\nEpoch [00380/1000], Loss: 0.525828\nValidation loss: 2.720078\nEpoch [00390/1000], Loss: 0.344514\nValidation loss: 2.773864\nEpoch [00400/1000], Loss: 0.489942\nValidation loss: 2.746784\nEpoch [00410/1000], Loss: 0.525823\nValidation loss: 2.733503\nEpoch [00420/1000], Loss: 0.400840\nValidation loss: 2.881040\nEpoch [00430/1000], Loss: 0.325227\nValidation loss: 2.807274\nEpoch [00440/1000], Loss: 0.315681\nValidation loss: 2.841484\nEpoch [00450/1000], Loss: 0.333904\nValidation loss: 2.885697\nEpoch [00460/1000], Loss: 0.406712\nValidation loss: 2.825935\nEpoch [00470/1000], Loss: 0.322010\nValidation loss: 2.774667\nEpoch [00480/1000], Loss: 0.322990\nValidation loss: 2.837084\nEpoch [00490/1000], Loss: 0.409554\nValidation loss: 2.924012\nEpoch [00500/1000], Loss: 0.352157\nValidation loss: 2.887895\nEpoch [00510/1000], Loss: 0.342611\nValidation loss: 2.893932\nEpoch [00520/1000], Loss: 0.362799\nValidation loss: 2.913622\nEpoch [00530/1000], Loss: 0.269409\nValidation loss: 2.939797\nEpoch [00540/1000], Loss: 0.225078\nValidation loss: 2.944119\nEpoch [00550/1000], Loss: 0.339646\nValidation loss: 2.972316\nEpoch [00560/1000], Loss: 0.288828\nValidation loss: 2.943407\nEpoch [00570/1000], Loss: 0.288510\nValidation loss: 2.967809\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}