{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81000,"databundleVersionId":8812083,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook we test a set of simple neural networks to predict crop yield from climate data.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:09:51.015715Z","iopub.execute_input":"2025-11-17T19:09:51.015959Z","iopub.status.idle":"2025-11-17T19:09:51.299569Z","shell.execute_reply.started":"2025-11-17T19:09:51.015939Z","shell.execute_reply":"2025-11-17T19:09:51.298886Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/the-future-crop-challenge/pr_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/sample_submission.csv\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_maize.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/pr_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/pr_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/train_solutions_wheat.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/soil_co2_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_maize_train.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmin_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/tas_maize_test.parquet\n/kaggle/input/the-future-crop-challenge/tasmax_wheat_test.parquet\n/kaggle/input/the-future-crop-challenge/rsds_wheat_train.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Dataset setup\n\nWe will try to train different architectures using pytorch. We spend some time here defining a dataset class.\nThis will let us train both recurrent networks and transformers later.","metadata":{}},{"cell_type":"code","source":"# Setup #\nimport os\nimport copy\nimport random\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import StandardScaler\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDATA_DIR = r'/kaggle/input/the-future-crop-challenge'\nprint(f'Running on {device}')\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    return\n\nsetup_seed(2025) #set seed to current year","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:09:53.950041Z","iopub.execute_input":"2025-11-17T19:09:53.950421Z","iopub.status.idle":"2025-11-17T19:09:58.036110Z","shell.execute_reply.started":"2025-11-17T19:09:53.950399Z","shell.execute_reply":"2025-11-17T19:09:58.035221Z"}},"outputs":[{"name":"stdout","text":"Running on cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class PredDataset(Dataset):\n    \"\"\"\n    Optimized dataset class that processes data in bulk using vectorized operations\n    and minimizes device transfers.\n    \"\"\"\n    def __init__(self, crop: str, mode: str, data_dir: str, device=device):\n        # Read all data files\n        tasmax = pd.read_parquet(os.path.join(data_dir, f\"tasmax_{crop}_{mode}.parquet\"))\n        tasmin = pd.read_parquet(os.path.join(data_dir, f\"tasmin_{crop}_{mode}.parquet\"))\n        pr = pd.read_parquet(os.path.join(data_dir, f\"pr_{crop}_{mode}.parquet\"))\n        rsds = pd.read_parquet(os.path.join(data_dir, f\"rsds_{crop}_{mode}.parquet\"))\n        soil_co2 = pd.read_parquet(os.path.join(data_dir, f\"soil_co2_{crop}_{mode}.parquet\"))\n        \n        # Load yield data if in training mode\n        if mode == 'train':\n            self.yield_data = pd.read_parquet(os.path.join(data_dir, f\"{mode}_solutions_{crop}.parquet\"))\n        else:\n            self.yield_data = None\n        # Preprocess climate data in bulk (shape: num_samples × 240 × 4)\n        climate_data = np.stack([\n            tasmax.iloc[:, 5:].values,\n            tasmin.iloc[:, 5:].values,\n            pr.iloc[:, 5:].values,\n            rsds.iloc[:, 5:].values\n        ], axis=2).astype(np.float32)\n        \n        # Preprocess soil data in bulk\n        soil_continuous = soil_co2[['lon', 'lat', 'co2', 'nitrogen']].values.astype(np.float32)\n        texture_classes = soil_co2['texture_class'].values.astype(np.int64) - 1\n        texture_one_hot = np.eye(13)[texture_classes].astype(np.float32)  # One-hot encoding\n        \n        # Combine soil features (shape: num_samples × 17)\n        soil_features = np.hstack([soil_continuous, texture_one_hot])\n        \n        # Expand soil features to match time dimension (shape: num_samples × 240 × 17)\n        soil_expanded = np.repeat(soil_features[:, np.newaxis, :], 240, axis=1)\n        \n        # Combine climate and soil features (shape: num_samples × 240 × 21)\n        full_input = np.concatenate([climate_data, soil_expanded], axis=2)\n        \n        # Move entire dataset to device in one operation\n        self.inputs = torch.tensor(full_input, device=device)\n        \n        # Process targets if available\n        self.targets = None\n        if self.yield_data is not None:\n            self.targets = torch.tensor(\n                self.yield_data.values.astype(np.float32), \n                device=device\n            )\n\n    def __getitem__(self, index):\n        # Return precomputed tensors\n        return self.inputs[index], self.targets[index]\n\n    def __len__(self):\n        return len(self.inputs)\n\nprint('Loading training data.,. (this may take a while)')\nds_wheat = PredDataset('wheat','train', DATA_DIR)\n\n#train_wheat = torch.utils.data.subset\n\n#print('Loading test data... (this may take a while)')\n#wheat_test = PredDataset('wheat','test', DATA_DIR, device = 'cpu')\n\nprint('Finished loading')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:10:01.088343Z","iopub.execute_input":"2025-11-17T19:10:01.089195Z","iopub.status.idle":"2025-11-17T19:10:24.321629Z","shell.execute_reply.started":"2025-11-17T19:10:01.089168Z","shell.execute_reply":"2025-11-17T19:10:24.320691Z"}},"outputs":[{"name":"stdout","text":"Loading training data.,. (this may take a while)\nFinished loading\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"n_train = int(len(ds_wheat.inputs)*0.8)\nn_val = len(ds_wheat.inputs)-n_train\n\ntrain_wheat = torch.utils.data.Subset(ds_wheat, range(n_train))\nval_wheat = torch.utils.data.Subset(ds_wheat,range(n_train,n_train+n_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:10:24.322848Z","iopub.execute_input":"2025-11-17T19:10:24.323071Z","iopub.status.idle":"2025-11-17T19:10:24.329077Z","shell.execute_reply.started":"2025-11-17T19:10:24.323053Z","shell.execute_reply":"2025-11-17T19:10:24.328198Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## simple RNN class\n\nHere's an example of a very simple RNN. As an auxilliary loss (hidden weight regularisation), we also try to predict the next crop data.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define model #\n\nclass PredictiveRNN(nn.Module):\n    def __init__(self, n_inputs, n_hidden, n_targets, n_layers, nonlinearity = 'relu'):\n        super(PredictiveRNN, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_inputs = n_inputs\n        self.n_targets = n_targets\n        self.n_layers = n_layers\n\n        self.rnn = nn.GRU(n_inputs, n_hidden, n_layers,\n                          batch_first = True)\n        self.encoder = nn.Linear(n_inputs, n_hidden*n_layers)\n        self.decoder = nn.Linear(n_hidden, n_targets)\n        self.mse = nn.MSELoss()\n\n        #initialise weights for improved gradient descent\n        for name, param in self.named_parameters():\n            if 'weight' in name:\n                if nonlinearity == 'tanh':\n                    nn.init.xavier_uniform_(param)\n                elif nonlinearity == 'relu':\n                    nn.init.kaiming_uniform_(param)           \n        \n    def forward(self, inputs):\n        #we want to initialise the hidden state with encoder weights rather than 0s\n        #these are esentially context-setting weights.\n        h0 = self.encoder(inputs[:,0,:])\n        h0 = h0.reshape(self.n_layers,inputs.shape[0],self.n_hidden) #(1,batch_size,1)\n       \n        rnn_out, hidden = self.rnn(inputs, h0)\n        predictions = self.decoder(rnn_out)\n\n        return predictions, rnn_out\n\n    def compute_losses(self, predictions, climate_targets, yield_target):\n        pred_loss = self.mse(predictions[:,:-1,:self.n_targets-1], #(n_batches, n_seq-1, n_targets-1 ) aligned to time t\n                             climate_targets[:,1:,:self.n_targets-1]) #(n_batches, n_seq-1, n_targets-1 ) aligned to time t+1\n        yield_loss = self.mse(predictions[:,-2:-1,-1], yield_target.expand(predictions[:,-2:-1,-1].shape)) #(n_batches,240,1)\n        return pred_loss, yield_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T18:55:36.532385Z","iopub.execute_input":"2025-11-08T18:55:36.532677Z","iopub.status.idle":"2025-11-08T18:55:36.540016Z","shell.execute_reply.started":"2025-11-08T18:55:36.532655Z","shell.execute_reply":"2025-11-08T18:55:36.539312Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Training loop #\n\n# global variables / options #\nBATCH_SIZE = 100\nN_EPOCHS = 2\nLEARNING_RATE = 1e-2\nN_HIDDEN = 512\nN_LAYERS = 1\nN_INPUT = 4+17 #4 climate datapoints, 17 soil datapoints (one-hot)\nN_TARGET = 4+1 #4 climate datapoints + crop yield\n\n# set up model and optimizer # \nmodel = PredictiveRNN(N_INPUT, N_HIDDEN, N_TARGET,N_LAYERS,\n                     nonlinearity = 'tanh').to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, amsgrad=True)\n\n# prepare data loader and start training loop #\n\ntrain_loader = DataLoader(train_wheat, batch_size= BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(val_wheat, batch_size= BATCH_SIZE, shuffle=False)\nprint('train loader success')\ndef train_model(train_loader, model, optimizer):\n    losses = {'train_or_test':[],'pred':[], 'yield':[],'epoch':[],'idx':[]}\n    for epoch in range(N_EPOCHS):\n        for i, (inputs, targets) in tqdm(enumerate(train_loader)):\n            optimizer.zero_grad()\n            #run data through model and backprop losses\n            predictions, _ = model(inputs)\n            pred_loss, yield_loss = model.compute_losses(predictions, inputs, targets)\n            loss = pred_loss+yield_loss\n            loss.backward()\n            optimizer.step()\n            #append losses\n            losses['train_or_test'].append('train')\n            losses['epoch'].append(epoch)\n            losses['yield'].append(yield_loss.detach().cpu().item())\n            losses['pred'].append(pred_loss.detach().cpu().item())\n            losses['idx'].append(i)\n            #print(f'Prediction loss: {round(prediction_losses[-1],3)}, yield loss: {round(yield_losses[-1],3)}')\n\n        with torch.no_grad():\n            for i, (inputs, targets) in tqdm(enumerate(test_loader)):\n                predictions, _  =model(inputs)\n                pred_loss, yield_loss = model.compute_losses(predictions, inputs, targets)\n                losses['train_or_test'].append('test')\n                losses['epoch'].append(epoch)\n                losses['yield'].append(yield_loss.detach().cpu().item())\n                losses['pred'].append(pred_loss.detach().cpu().item())\n                losses['idx'].append(i)\n\n    return model, losses\n\nmodel, losses = train_model(train_loader, model,optimizer)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:48:57.816064Z","iopub.execute_input":"2025-11-08T19:48:57.816334Z","iopub.status.idle":"2025-11-08T19:53:45.794416Z","shell.execute_reply.started":"2025-11-08T19:48:57.816314Z","shell.execute_reply":"2025-11-08T19:53:45.793671Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"train loader success\n","output_type":"stream"},{"name":"stderr","text":"2230it [02:11, 16.92it/s]\n558it [00:13, 42.21it/s]\n2230it [02:09, 17.20it/s]\n558it [00:13, 42.12it/s]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"loss_df.groupby(['epoch','train_or_test']).mean('yield')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:53:52.348605Z","iopub.execute_input":"2025-11-08T19:53:52.349238Z","iopub.status.idle":"2025-11-08T19:53:52.361083Z","shell.execute_reply.started":"2025-11-08T19:53:52.349213Z","shell.execute_reply":"2025-11-08T19:53:52.360465Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"                           pred      yield     idx\nepoch train_or_test                               \n0     test           500.091660   2.715062   278.5\n      train          611.086898  20.362136  1114.5\n1     test           466.304636   1.829468   278.5\n      train          480.478830   2.076788  1114.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>pred</th>\n      <th>yield</th>\n      <th>idx</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th>train_or_test</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>test</th>\n      <td>500.091660</td>\n      <td>2.715062</td>\n      <td>278.5</td>\n    </tr>\n    <tr>\n      <th>train</th>\n      <td>611.086898</td>\n      <td>20.362136</td>\n      <td>1114.5</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>test</th>\n      <td>466.304636</td>\n      <td>1.829468</td>\n      <td>278.5</td>\n    </tr>\n    <tr>\n      <th>train</th>\n      <td>480.478830</td>\n      <td>2.076788</td>\n      <td>1114.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"loss_df = pd.DataFrame(losses)\ngrouped_df = loss_df.groupby(['epoch','train_or_test']).mean('yield')\nimport seaborn as sns\nfig, ax = plt.subplots(1,2, figsize = (10,5))\nax[0].loglog(loss_df.query('train_or_test == \"train\"')['yield'])\nax[0].loglog(loss_df.query('train_or_test == \"test\"')['yield'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:53:56.622765Z","iopub.execute_input":"2025-11-08T19:53:56.623574Z","iopub.status.idle":"2025-11-08T19:53:57.240487Z","shell.execute_reply.started":"2025-11-08T19:53:56.623542Z","shell.execute_reply":"2025-11-08T19:53:57.239651Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7bfc62ad07d0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":75},{"cell_type":"markdown","source":"Note: I've tested different network sizes and 2 layers.\n\nLarger and deeper networks take longer to train and don't seem to have much different performance. The current estimate of yield loss varies around 1.5\n\nThat said, we should do a better estimate of network performance using cross-validated held-out data from the last few years of the dataset (having to generalise to unseen data with higher CO2 concentrations, as in the actual test of the competition).\n\nI've also tried GRU's and LSTMs with size 512, these obtaining yield losses varying around [1.2] (prediction loss round 360)\n","metadata":{}},{"cell_type":"code","source":"fix, ax  = plt.subplots(1,2)\nax[0].loglog(yield_losses)\nax[1].loglog(prediction_losses)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Make sure the code is as efficient as can be:\n# %load_ext line_profiler\n# %lprun -f train_model train_model(train_loader, model,optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:46:36.510719Z","iopub.status.idle":"2025-09-26T17:46:36.511035Z","shell.execute_reply.started":"2025-09-26T17:46:36.510882Z","shell.execute_reply":"2025-09-26T17:46:36.510898Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training transformers\n\nAssuming this will have larger memory demands, though.\n\nI've asked claude to make a flexible PredNet class","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport math\n\n\n\nclass FlexiblePredNet(nn.Module):\n    def __init__(self, n_inputs, n_hidden, n_targets, n_layers, \n                 model_type='gru', nonlinearity='tanh', \n                 n_heads=8, dropout=0.1, sequence_length=None):\n        \"\"\"\n        Flexible predictive network supporting RNN, GRU, and Transformer architectures\n        \n        Args:\n            n_inputs: Input feature dimension per timestep\n            n_hidden: Hidden dimension\n            n_targets: Output target dimension per timestep\n            n_layers: Number of layers\n            model_type: 'rnn', 'gru', or 'transformer'\n            nonlinearity: 'tanh' or 'relu' (for RNN/GRU)\n            n_heads: Number of attention heads (for transformer)\n            dropout: Dropout rate (for transformer)\n            sequence_length: Required for transformer - length of input sequences\n        \"\"\"\n        super(FlexiblePredNet, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_inputs = n_inputs\n        self.n_targets = n_targets\n        self.n_layers = n_layers\n        self.model_type = model_type.lower()\n        self.sequence_length = sequence_length\n        \n        # Model-specific components\n        if self.model_type == 'rnn':\n            # Input projection to hidden dimension (per timestep)\n            self.input_projection = nn.Linear(n_inputs, n_hidden)\n            self.sequence_model = nn.RNN(n_hidden, n_hidden, n_layers,\n                                       batch_first=True, nonlinearity=nonlinearity)\n            self.encoder = nn.Linear(n_inputs, n_hidden * n_layers)\n            # Output projection (per timestep)\n            self.decoder = nn.Linear(n_hidden, n_targets)\n            \n        elif self.model_type == 'gru':\n            # Input projection to hidden dimension (per timestep)\n            self.input_projection = nn.Linear(n_inputs, n_hidden)\n            self.sequence_model = nn.GRU(n_hidden, n_hidden, n_layers,\n                                       batch_first=True)\n            self.encoder = nn.Linear(n_inputs, n_hidden * n_layers)\n            # Output projection (per timestep)\n            self.decoder = nn.Linear(n_hidden, n_targets)\n            \n        elif self.model_type == 'transformer':\n            if sequence_length is None:\n                raise ValueError(\"sequence_length must be provided for transformer model\")\n            \n            # Calculate flattened input size: sequence_length * climate_features + soil_features\n            # Assuming last 17 features are soil (one-hot), first 4 are climate per timestep\n            climate_features = n_inputs - 17  # 4 climate features\n            soil_features = 17  # 17 soil features\n            flattened_input_size = sequence_length * climate_features + soil_features\n            \n            # Input projection to transformer dimension\n            self.input_projection = nn.Linear(flattened_input_size, n_hidden)\n            \n            # Standard transformer encoder\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=n_hidden,\n                nhead=n_heads,\n                dim_feedforward=n_hidden //2,\n                dropout=dropout,\n                activation='relu',\n                batch_first=True\n            )\n            self.sequence_model = nn.TransformerEncoder(encoder_layer, n_layers)\n            \n            # Output projection to full sequence\n            self.decoder = nn.Linear(n_hidden, sequence_length * n_targets)\n            \n        else:\n            raise ValueError(f\"Unsupported model_type: {model_type}. Choose from 'rnn', 'gru', 'transformer'\")\n        \n        self.mse = nn.MSELoss()\n        \n        # Initialize weights\n        if model_type != 'transformer':\n            self._initialize_weights(nonlinearity)\n        \n    def _initialize_weights(self, nonlinearity):\n        \"\"\"Initialize model weights\"\"\"\n        for name, param in self.named_parameters():\n            if 'weight' in name and 'decoder' not in name:\n                if nonlinearity == 'tanh':\n                    nn.init.xavier_uniform_(param)\n                elif nonlinearity == 'relu':\n                    nn.init.kaiming_uniform_(param)\n                else:\n                    nn.init.xavier_uniform_(param)  # Default for transformer\n                    \n    def _create_causal_mask(self, seq_len, device):\n        \"\"\"Create causal (lower triangular) mask for transformer\"\"\"\n        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n        \n    def forward(self, inputs):\n        batch_size, seq_len, input_dim = inputs.shape\n        \n        if self.model_type in ['rnn', 'gru']:\n            # Project inputs to hidden dimension\n            projected_inputs = self.input_projection(inputs)\n            \n            # Initialize hidden state with encoder weights (like your original)\n            h0 = self.encoder(inputs[:, 0, :])\n            h0 = h0.reshape(self.n_layers, batch_size, self.n_hidden)\n            \n            # Run through RNN/GRU\n            sequence_out, hidden = self.sequence_model(projected_inputs, h0)\n            \n            # Decode to output dimension\n            predictions = self.decoder(sequence_out)\n            return predictions, sequence_out\n            \n        elif self.model_type == 'transformer':\n            # Flatten the sequence for transformer\n            # Split climate (first 4 features per timestep) and soil (last 17 features)\n            climate_features = 4\n            soil_features = 17\n            \n            # Extract climate features from all timesteps: (batch, seq_len, 4)\n            climate_data = inputs[:, :, :climate_features]  \n            # Extract soil features from last timestep: (batch, 17)\n            soil_data = inputs[:, -1, climate_features:]  \n            \n            # Flatten climate data: (batch, seq_len * 4)\n            flattened_climate = climate_data.reshape(batch_size, -1)\n            \n            # Concatenate flattened climate with soil: (batch, seq_len * 4 + 17)\n            flattened_input = torch.cat([flattened_climate, soil_data], dim=1)\n            \n            # Project to transformer dimension and add sequence dimension for transformer\n            # Transformer expects (batch, seq, features), so we add a dummy sequence dimension\n            projected_input = self.input_projection(flattened_input).unsqueeze(1)  # (batch, 1, hidden)\n            \n            # Create causal mask (though with seq_len=1, this is not strictly needed)\n            causal_mask = self._create_causal_mask(1, inputs.device)\n            \n            # Pass through transformer\n            transformer_out = self.sequence_model(projected_input, mask=causal_mask)  # (batch, 1, hidden)\n            \n            # Decode to full sequence output\n            flat_predictions = self.decoder(transformer_out.squeeze(1))  # (batch, seq_len * n_targets)\n            \n            # Reshape back to sequence format: (batch, seq_len, n_targets)\n            predictions = flat_predictions.reshape(batch_size, seq_len, self.n_targets)\n            \n            return predictions, transformer_out.squeeze(1)\n    \n    def compute_losses(self, predictions, climate_targets, yield_target):\n        \"\"\"Compute prediction and yield losses with causal masking for loss computation\"\"\"\n        if self.model_type in ['rnn', 'gru']:\n            # Original loss computation for RNN/GRU\n            pred_loss = self.mse(predictions[:, :-1, :self.n_targets-1], \n                               climate_targets[:, 1:, :self.n_targets-1])\n            yield_loss = self.mse(predictions[:, -2:-1, -1], \n                                yield_target.expand(predictions[:, -2:-1, -1].shape))\n        \n        elif self.model_type == 'transformer':\n            # For transformer, we can apply causal masking to the loss computation\n            # This ensures we only compute loss where we should have predictions\n            batch_size, seq_len, _ = predictions.shape\n            \n            # Create causal mask for loss computation\n            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=predictions.device))\n            \n            # Compute prediction loss with causal masking\n            pred_losses = []\n            for t in range(1, seq_len):  # Start from 1 since we predict t+1 from t\n                # At timestep t, we can predict based on timesteps 0 to t-1\n                pred_at_t = predictions[:, t-1:t, :self.n_targets-1]  # Prediction made at t-1\n                target_at_t = climate_targets[:, t:t+1, :self.n_targets-1]  # Target at t\n                pred_losses.append(self.mse(pred_at_t, target_at_t))\n            \n            pred_loss = torch.stack(pred_losses).mean()\n            \n            # Yield loss (predict from second-to-last timestep)\n            yield_loss = self.mse(predictions[:, -2:-1, -1], \n                                yield_target.expand(predictions[:, -2:-1, -1].shape))\n        \n        return pred_loss, yield_loss\n\n# Updated training function with model type logging\ndef train_model(train_loader, val_loader, model, optimizer, n_epochs, model_type):\n    losses  = {'train':{'yield':[],'pred':[]},'val':{'yield':[],'pred':[]}}\n    \n    print(f\"Training {model_type.upper()} model...\")\n    \n    for epoch in range(n_epochs):\n        epoch_yield_losses = []\n        epoch_pred_losses = []\n        \n        for i, (inputs, targets) in tqdm(enumerate(train_loader), \n                                        desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n            optimizer.zero_grad()\n            \n            # Run data through model and backprop losses\n            predictions, _ = model(inputs)\n            pred_loss, yield_loss = model.compute_losses(predictions, inputs, targets)\n            loss = yield_loss # + 1e-2*pred_loss\n            loss.backward()\n            optimizer.step()\n            \n            # Append losses\n            epoch_yield_losses.append(yield_loss.detach().cpu().item())\n            epoch_pred_losses.append(pred_loss.detach().cpu().item())\n        \n        # Store epoch averages\n        losses['train']['yield'].extend(epoch_yield_losses)\n        losses['train']['pred'].extend(epoch_pred_losses)\n        \n        avg_pred = sum(epoch_pred_losses) / len(epoch_pred_losses)\n        avg_yield = sum(epoch_yield_losses) / len(epoch_yield_losses)\n\n        if (epoch % 10) == 0:   \n            val_yield_losses = []\n            with torch.no_grad():\n                model.eval()\n                for inputs, targets in val_loader:\n                # Run data through model and backprop losses\n                    predictions, _ = model(inputs)\n                    pred_loss, yield_loss = model.compute_losses(predictions, inputs, targets)\n                    val_yield_losses.append(yield_loss.item())\n            val_yield = np.mean(val_yield_losses)\n            losses['val']['yield'].extend(val_yield_losses)\n            print(f'Epoch {epoch+1}: Prediction loss: {avg_pred:.3f}, Yield loss: {avg_yield:.3f} \\n Val loss: {val_yield:.3f}')\n            \n    return model, losses\n\n# Usage example with different model types\ndef train_and_compare_models(train_loader, val_loader, device, n_epochs=10):\n    \"\"\"Train and compare different model architectures\"\"\"\n    results = {}\n    model_configs = [\n        #{'type': 'rnn', 'nonlinearity': 'tanh'},\n        {'type': 'transformer', 'dropout': 0.2},\n       # {'type': 'gru', 'nonlinearity': 'tanh'},\n    ]\n    \n    for config in model_configs:\n        print(f\"\\n{'='*50}\")\n        model_type = config['type']\n        print(f\"Training {model_type.upper()} model\")\n        print(f\"{'='*50}\")\n        \n        # Create model\n        if model_type == 'transformer':\n            model = FlexiblePredNet(\n                N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                model_type=model_type,\n                dropout=config['dropout'],\n                sequence_length=240  # You'll need to set this to your actual sequence length\n            ).to(device)\n        else:\n            model = FlexiblePredNet(\n                N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                model_type=model_type,\n                nonlinearity=config['nonlinearity']\n            ).to(device)\n        \n        # Create optimizer\n        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, \n                                      amsgrad=True, weight_decay = 1e-3)\n        \n        # Train model\n        trained_model, losses = train_model(\n            train_loader,val_loader, model, optimizer, n_epochs, model_type\n        )\n        \n        results[model_type] = {\n            'model': trained_model,\n            'losses': losses\n        }\n    \n    return results\n\n# Simple usage example for single model\nif __name__ == \"__main__\":\n    # Example usage - replace with your actual data loader and device\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Global variables / options\n    BATCH_SIZE = 100\n    N_EPOCHS = 100\n    LEARNING_RATE = 3e-4\n    N_HIDDEN = 128\n    N_LAYERS = 2\n    N_INPUT = 4 + 17  # 4 climate datapoints, 17 soil datapoints (one-hot)\n    N_TARGET = 4 + 1  # 4 climate datapoints + crop yield\n    \n    # Example: Create a GRU model (same as your original)\n    model_gru = FlexiblePredNet(N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                               model_type='gru', nonlinearity='tanh')\n    \n    # Example: Create a Transformer model without positional encoding\n    model_transformer = FlexiblePredNet(N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                                      model_type='transformer', dropout=0.1, \n                                      sequence_length=240)  # Specify your sequence length\n    \n    # Example: Create an RNN model\n    model_rnn = FlexiblePredNet(N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                              model_type='rnn', nonlinearity='tanh')\n    \n    print(\"Models created successfully!\")\n    print(f\"GRU parameters: {sum(p.numel() for p in model_gru.parameters()):,}\")\n    print(f\"Transformer parameters: {sum(p.numel() for p in model_transformer.parameters()):,}\")\n    print(f\"RNN parameters: {sum(p.numel() for p in model_rnn.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:31:21.231366Z","iopub.execute_input":"2025-11-14T19:31:21.231856Z","iopub.status.idle":"2025-11-14T19:31:21.269329Z","shell.execute_reply.started":"2025-11-14T19:31:21.231831Z","shell.execute_reply":"2025-11-14T19:31:21.268642Z"}},"outputs":[{"name":"stdout","text":"Models created successfully!\nGRU parameters: 207,237\nTransformer parameters: 446,256\nRNN parameters: 75,141\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport math\n\n\nclass RevIN(nn.Module):\n    \"\"\"\n    Reversible Instance Normalization for time series\n    Normalizes input by instance statistics, then denormalizes output\n    \"\"\"\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(RevIN, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        \n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n    \n    def forward(self, x, mode='norm'):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch, seq_len, features)\n            mode: 'norm' for normalization, 'denorm' for denormalization\n        \"\"\"\n        if mode == 'norm':\n            # Store statistics for denormalization\n            self._get_statistics(x)\n            # Normalize\n            x = x - self.mean\n            x = x / (self.stdev + self.eps)\n            \n            if self.affine:\n                x = x * self.affine_weight\n                x = x + self.affine_bias\n            return x\n            \n        elif mode == 'denorm':\n            # Denormalize\n            if self.affine:\n                x = x - self.affine_bias\n                x = x / (self.affine_weight + self.eps * self.eps)\n            \n            x = x * (self.stdev + self.eps)\n            x = x + self.mean\n            return x\n        else:\n            raise NotImplementedError(f\"Mode {mode} not supported\")\n    \n    def _get_statistics(self, x):\n        \"\"\"Compute mean and stdev across sequence length dimension\"\"\"\n        # x shape: (batch, seq_len, features)\n        # Compute statistics along sequence dimension (dim=1)\n        self.mean = torch.mean(x, dim=1, keepdim=True).detach()\n        self.stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + self.eps).detach()\n\n\nclass FlexiblePredNet(nn.Module):\n    def __init__(self, n_inputs, n_hidden, n_targets, n_layers, \n                 model_type='gru', nonlinearity='tanh', \n                 n_heads=8, dropout=0.1, sequence_length=None,\n                 use_revin=True, revin_affine=True):\n        \"\"\"\n        Flexible predictive network supporting RNN, GRU, and Transformer architectures\n        \n        Args:\n            n_inputs: Input feature dimension per timestep\n            n_hidden: Hidden dimension\n            n_targets: Output target dimension per timestep\n            n_layers: Number of layers\n            model_type: 'rnn', 'gru', or 'transformer'\n            nonlinearity: 'tanh' or 'relu' (for RNN/GRU)\n            n_heads: Number of attention heads (for transformer)\n            dropout: Dropout rate (for transformer)\n            sequence_length: Required for transformer - length of input sequences\n            use_revin: Whether to use Reversible Instance Normalization\n            revin_affine: Whether to use affine transformation in RevIN\n        \"\"\"\n        super(FlexiblePredNet, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_inputs = n_inputs\n        self.n_targets = n_targets\n        self.n_layers = n_layers\n        self.model_type = model_type.lower()\n        self.sequence_length = sequence_length\n        self.use_revin = use_revin\n        \n        # RevIN for climate features only (first 4 features)\n        # We don't normalize soil features since they're one-hot encoded\n        if self.use_revin:\n            self.revin_climate = RevIN(4, affine=revin_affine)  # Only for 4 climate features\n        \n        # Model-specific components\n        if self.model_type == 'rnn':\n            # Input projection to hidden dimension (per timestep)\n            self.input_projection = nn.Linear(n_inputs, n_hidden)\n            self.sequence_model = nn.RNN(n_hidden, n_hidden, n_layers,\n                                       batch_first=True, nonlinearity=nonlinearity)\n            self.encoder = nn.Linear(n_inputs, n_hidden * n_layers)\n            # Output projection (per timestep)\n            self.decoder = nn.Linear(n_hidden, n_targets)\n            \n        elif self.model_type == 'gru':\n            # Input projection to hidden dimension (per timestep)\n            self.input_projection = nn.Linear(n_inputs, n_hidden)\n            self.sequence_model = nn.GRU(n_hidden, n_hidden, n_layers,\n                                       batch_first=True)\n            self.encoder = nn.Linear(n_inputs, n_hidden * n_layers)\n            # Output projection (per timestep)\n            self.decoder = nn.Linear(n_hidden, n_targets)\n            \n        elif self.model_type == 'transformer':\n            if sequence_length is None:\n                raise ValueError(\"sequence_length must be provided for transformer model\")\n            \n            # Calculate flattened input size: sequence_length * climate_features + soil_features\n            # Assuming last 17 features are soil (one-hot), first 4 are climate per timestep\n            climate_features = n_inputs - 17  # 4 climate features\n            soil_features = 17  # 17 soil features\n            flattened_input_size = sequence_length * climate_features + soil_features\n            \n            # Input projection to transformer dimension\n            self.input_projection = nn.Linear(flattened_input_size, n_hidden)\n            \n            # Standard transformer encoder\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=n_hidden,\n                nhead=n_heads,\n                dim_feedforward=n_hidden // 2,  # 0.5x bottleneck ratio\n                dropout=dropout,\n                activation='relu',\n                batch_first=True\n            )\n            self.sequence_model = nn.TransformerEncoder(encoder_layer, n_layers)\n            \n            # Output projection to full sequence\n            self.decoder = nn.Linear(n_hidden, sequence_length * n_targets)\n            \n        else:\n            raise ValueError(f\"Unsupported model_type: {model_type}. Choose from 'rnn', 'gru', 'transformer'\")\n        \n        self.mse = nn.MSELoss()\n        \n        # Initialize weights\n        if model_type != 'transformer':\n            self._initialize_weights(nonlinearity)\n        \n    def _initialize_weights(self, nonlinearity):\n        \"\"\"Initialize model weights\"\"\"\n        for name, param in self.named_parameters():\n            # Skip RevIN parameters and 1D tensors\n            if 'revin' in name or param.dim() < 2:\n                continue\n            if 'weight' in name and 'decoder' not in name:\n                if nonlinearity == 'tanh':\n                    nn.init.xavier_uniform_(param)\n                elif nonlinearity == 'relu':\n                    nn.init.kaiming_uniform_(param)\n                else:\n                    nn.init.xavier_uniform_(param)  # Default for transformer\n                    \n    def _create_causal_mask(self, seq_len, device):\n        \"\"\"Create causal (lower triangular) mask for transformer\"\"\"\n        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n        \n    def forward(self, inputs):\n        batch_size, seq_len, input_dim = inputs.shape\n        \n        # Apply RevIN to climate features if enabled\n        if self.use_revin:\n            # Split climate and soil features\n            climate_features = inputs[:, :, :4]  # First 4 features\n            soil_features = inputs[:, :, 4:]     # Last 17 features\n            \n            # Normalize climate features\n            climate_normalized = self.revin_climate(climate_features, mode='norm')\n            \n            # Recombine\n            inputs = torch.cat([climate_normalized, soil_features], dim=-1)\n        \n        if self.model_type in ['rnn', 'gru']:\n            # Project inputs to hidden dimension\n            projected_inputs = self.input_projection(inputs)\n            \n            # Initialize hidden state with encoder weights (like your original)\n            h0 = self.encoder(inputs[:, 0, :])\n            h0 = h0.reshape(self.n_layers, batch_size, self.n_hidden)\n            \n            # Run through RNN/GRU\n            sequence_out, hidden = self.sequence_model(projected_inputs, h0)\n            \n            # Decode to output dimension\n            predictions = self.decoder(sequence_out)\n            \n            # Denormalize climate predictions if RevIN is enabled\n            if self.use_revin:\n                # Split predictions into climate and yield\n                climate_predictions = predictions[:, :, :4]  # First 4 features\n                yield_predictions = predictions[:, :, 4:]    # Last feature (yield)\n                \n                # Denormalize climate predictions\n                climate_denormalized = self.revin_climate(climate_predictions, mode='denorm')\n                \n                # Recombine\n                predictions = torch.cat([climate_denormalized, yield_predictions], dim=-1)\n            \n            return predictions, sequence_out\n            \n        elif self.model_type == 'transformer':\n            # Flatten the sequence for transformer\n            # Split climate (first 4 features per timestep) and soil (last 17 features)\n            climate_features = 4\n            soil_features = 17\n            \n            # Extract climate features from all timesteps: (batch, seq_len, 4)\n            climate_data = inputs[:, :, :climate_features]  \n            # Extract soil features from last timestep: (batch, 17)\n            soil_data = inputs[:, -1, climate_features:]  \n            \n            # Flatten climate data: (batch, seq_len * 4)\n            flattened_climate = climate_data.reshape(batch_size, -1)\n            \n            # Concatenate flattened climate with soil: (batch, seq_len * 4 + 17)\n            flattened_input = torch.cat([flattened_climate, soil_data], dim=1)\n            \n            # Project to transformer dimension and add sequence dimension for transformer\n            # Transformer expects (batch, seq, features), so we add a dummy sequence dimension\n            projected_input = self.input_projection(flattened_input).unsqueeze(1)  # (batch, 1, hidden)\n            \n            # Create causal mask (though with seq_len=1, this is not strictly needed)\n            causal_mask = self._create_causal_mask(1, inputs.device)\n            \n            # Pass through transformer\n            transformer_out = self.sequence_model(projected_input, mask=causal_mask)  # (batch, 1, hidden)\n            \n            # Decode to full sequence output\n            flat_predictions = self.decoder(transformer_out.squeeze(1))  # (batch, seq_len * n_targets)\n            \n            # Reshape back to sequence format: (batch, seq_len, n_targets)\n            predictions = flat_predictions.reshape(batch_size, seq_len, self.n_targets)\n            \n            # Denormalize climate predictions if RevIN is enabled\n            if self.use_revin:\n                # Split predictions into climate and yield\n                climate_predictions = predictions[:, :, :4]  # First 4 features\n                yield_predictions = predictions[:, :, 4:]    # Last feature (yield)\n                \n                # Denormalize climate predictions\n                climate_denormalized = self.revin_climate(climate_predictions, mode='denorm')\n                \n                # Recombine\n                predictions = torch.cat([climate_denormalized, yield_predictions], dim=-1)\n            \n            return predictions, transformer_out.squeeze(1)\n    \n    def compute_losses(self, predictions, climate_targets, yield_target):\n        \"\"\"Compute prediction and yield losses with causal masking for loss computation\"\"\"\n        if self.model_type in ['rnn', 'gru']:\n            # Original loss computation for RNN/GRU\n            pred_loss = self.mse(predictions[:, :-1, :self.n_targets-1], \n                               climate_targets[:, 1:, :self.n_targets-1])\n            yield_loss = self.mse(predictions[:, -2:-1, -1], \n                                yield_target.expand(predictions[:, -2:-1, -1].shape))\n        \n        elif self.model_type == 'transformer':\n            # For transformer, we can apply causal masking to the loss computation\n            # This ensures we only compute loss where we should have predictions\n            batch_size, seq_len, _ = predictions.shape\n            \n            # Create causal mask for loss computation\n            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=predictions.device))\n            \n            # Compute prediction loss with causal masking\n            pred_losses = []\n            for t in range(1, seq_len):  # Start from 1 since we predict t+1 from t\n                # At timestep t, we can predict based on timesteps 0 to t-1\n                pred_at_t = predictions[:, t-1:t, :self.n_targets-1]  # Prediction made at t-1\n                target_at_t = climate_targets[:, t:t+1, :self.n_targets-1]  # Target at t\n                pred_losses.append(self.mse(pred_at_t, target_at_t))\n            \n            pred_loss = torch.stack(pred_losses).mean()\n            \n            # Yield loss (predict from second-to-last timestep)\n            yield_loss = self.mse(predictions[:, -2:-1, -1], \n                                yield_target.expand(predictions[:, -2:-1, -1].shape))\n        \n        return pred_loss, yield_loss\n\n\n# Updated training function with model type logging\ndef train_model(train_loader, model, optimizer, n_epochs, model_type):\n    yield_losses = []\n    prediction_losses = []\n    \n    print(f\"Training {model_type.upper()} model...\")\n    \n    for epoch in range(n_epochs):\n        epoch_yield_losses = []\n        epoch_pred_losses = []\n        \n        for i, (inputs, targets) in tqdm(enumerate(train_loader), \n                                        desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n            optimizer.zero_grad()\n            \n            # Run data through model and backprop losses\n            predictions, _ = model(inputs)\n            pred_loss, yield_loss = model.compute_losses(predictions, inputs, targets)\n            loss = yield_loss # + 1e-2*pred_loss\n            loss.backward()\n            optimizer.step()\n            \n            # Append losses\n            epoch_yield_losses.append(yield_loss.detach().cpu().item())\n            epoch_pred_losses.append(pred_loss.detach().cpu().item())\n            \n        # Store epoch averages\n        yield_losses.extend(epoch_yield_losses)\n        prediction_losses.extend(epoch_pred_losses)\n        \n        avg_pred = sum(epoch_pred_losses) / len(epoch_pred_losses)\n        avg_yield = sum(epoch_yield_losses) / len(epoch_yield_losses)\n        if epoch % 10 == 0:\n            print(f'Epoch {epoch+1}: Prediction loss: {avg_pred:.3f}, Yield loss: {avg_yield:.3f}')\n    \n    return model, yield_losses, prediction_losses\n\n\n# Usage example with different model types\ndef train_and_compare_models(train_loader, device, n_epochs=10):\n    \"\"\"Train and compare different model architectures\"\"\"\n    \n    results = {}\n    \n    model_configs = [\n        {'type': 'transformer', 'dropout': 0.2, 'use_revin': True},\n        {'type': 'transformer', 'dropout': 0.2, 'use_revin': False},  # For comparison\n    ]\n    \n    for config in model_configs:\n        print(f\"\\n{'='*50}\")\n        model_type = config['type']\n        use_revin = config.get('use_revin', True)\n        print(f\"Training {model_type.upper()} model (RevIN: {use_revin})\")\n        print(f\"{'='*50}\")\n        \n        # Create model\n        if model_type == 'transformer':\n            model = FlexiblePredNet(\n                N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                model_type=model_type,\n                dropout=config['dropout'],\n                sequence_length=240,\n                use_revin=use_revin\n            ).to(device)\n        else:\n            model = FlexiblePredNet(\n                N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                model_type=model_type,\n                nonlinearity=config['nonlinearity'],\n                use_revin=use_revin\n            ).to(device)\n        \n        # Create optimizer\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, amsgrad=True)\n        \n        # Train model\n        trained_model, yield_losses, prediction_losses = train_model(\n            train_loader, model, optimizer, n_epochs, model_type\n        )\n        \n        config_name = f\"{model_type}_revin_{use_revin}\"\n        results[config_name] = {\n            'model': trained_model,\n            'yield_losses': yield_losses,\n            'prediction_losses': prediction_losses\n        }\n    \n    return results\n\n\n# Simple usage example for single model\nif __name__ == \"__main__\":\n    # Example usage - replace with your actual data loader and device\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Global variables / options\n    BATCH_SIZE = 100\n    N_EPOCHS = 100\n    LEARNING_RATE = 1e-2\n    N_HIDDEN = 32\n    N_LAYERS = 2\n    N_INPUT = 4 + 17  # 4 climate datapoints, 17 soil datapoints (one-hot)\n    N_TARGET = 4 + 1  # 4 climate datapoints + crop yield\n    \n    # Example: Create a GRU model with RevIN\n    model_gru = FlexiblePredNet(N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                               model_type='gru', nonlinearity='tanh',\n                               use_revin=True)\n    \n    # Example: Create a Transformer model with RevIN and bottleneck MLP\n    model_transformer = FlexiblePredNet(N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                                      model_type='transformer', dropout=0.1, \n                                      sequence_length=240,\n                                      use_revin=True)\n    \n    # Example: Create an RNN model without RevIN (for comparison)\n    model_rnn = FlexiblePredNet(N_INPUT, N_HIDDEN, N_TARGET, N_LAYERS,\n                              model_type='rnn', nonlinearity='tanh',\n                              use_revin=False)\n    \n    print(\"Models created successfully!\")\n    print(f\"GRU parameters: {sum(p.numel() for p in model_gru.parameters()):,}\")\n    print(f\"Transformer parameters: {sum(p.numel() for p in model_transformer.parameters()):,}\")\n    print(f\"RNN parameters: {sum(p.numel() for p in model_rnn.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:41:56.271838Z","iopub.execute_input":"2025-11-17T19:41:56.272647Z","iopub.status.idle":"2025-11-17T19:41:56.310204Z","shell.execute_reply.started":"2025-11-17T19:41:56.272621Z","shell.execute_reply":"2025-11-17T19:41:56.309635Z"}},"outputs":[{"name":"stdout","text":"Models created successfully!\nGRU parameters: 14,957\nTransformer parameters: 81,752\nRNN parameters: 6,501\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_loader = DataLoader(train_wheat, batch_size = 2**10, shuffle = True)\nval_loader = DataLoader(val_wheat, batch_size = 512, shuffle = False)\n\nresults = train_and_compare_models(train_loader, 'cuda', n_epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:41:58.598419Z","iopub.execute_input":"2025-11-17T19:41:58.598976Z","iopub.status.idle":"2025-11-17T19:47:31.605228Z","shell.execute_reply.started":"2025-11-17T19:41:58.598953Z","shell.execute_reply":"2025-11-17T19:47:31.604575Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTraining TRANSFORMER model (RevIN: True)\n==================================================\nTraining TRANSFORMER model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 218it [00:08, 25.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Prediction loss: 2287.669, Yield loss: 1.959\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 218it [00:08, 25.98it/s]\nEpoch 3/20: 218it [00:08, 25.50it/s]\nEpoch 4/20: 218it [00:08, 26.01it/s]\nEpoch 5/20: 218it [00:08, 26.01it/s]\nEpoch 6/20: 218it [00:08, 25.48it/s]\nEpoch 7/20: 218it [00:08, 26.04it/s]\nEpoch 8/20: 218it [00:08, 25.35it/s]\nEpoch 9/20: 218it [00:08, 26.08it/s]\nEpoch 10/20: 218it [00:08, 26.30it/s]\nEpoch 11/20: 218it [00:08, 25.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Prediction loss: 1591.380, Yield loss: 0.914\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 218it [00:08, 26.27it/s]\nEpoch 13/20: 218it [00:08, 26.19it/s]\nEpoch 14/20: 218it [00:08, 25.82it/s]\nEpoch 15/20: 218it [00:08, 26.11it/s]\nEpoch 16/20: 218it [00:08, 26.17it/s]\nEpoch 17/20: 218it [00:08, 25.67it/s]\nEpoch 18/20: 218it [00:08, 26.33it/s]\nEpoch 19/20: 218it [00:08, 26.27it/s]\nEpoch 20/20: 218it [00:08, 25.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTraining TRANSFORMER model (RevIN: False)\n==================================================\nTraining TRANSFORMER model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 218it [00:07, 27.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Prediction loss: 5813.961, Yield loss: 2.564\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 218it [00:07, 28.10it/s]\nEpoch 3/20: 218it [00:07, 27.76it/s]\nEpoch 4/20: 218it [00:07, 28.11it/s]\nEpoch 5/20: 218it [00:08, 27.04it/s]\nEpoch 6/20: 218it [00:08, 25.78it/s]\nEpoch 7/20: 218it [00:08, 26.38it/s]\nEpoch 8/20: 218it [00:08, 26.02it/s]\nEpoch 9/20: 218it [00:08, 25.50it/s]\nEpoch 10/20: 218it [00:08, 26.17it/s]\nEpoch 11/20: 218it [00:08, 25.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Prediction loss: 5815.315, Yield loss: 1.510\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 218it [00:08, 26.34it/s]\nEpoch 13/20: 218it [00:08, 26.37it/s]\nEpoch 14/20: 218it [00:08, 25.81it/s]\nEpoch 15/20: 218it [00:08, 26.42it/s]\nEpoch 16/20: 218it [00:08, 26.16it/s]\nEpoch 17/20: 218it [00:08, 25.54it/s]\nEpoch 18/20: 218it [00:08, 26.07it/s]\nEpoch 19/20: 218it [00:08, 26.31it/s]\nEpoch 20/20: 218it [00:08, 26.39it/s]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"results.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:39:16.453756Z","iopub.execute_input":"2025-11-17T19:39:16.454541Z","iopub.status.idle":"2025-11-17T19:39:16.460049Z","shell.execute_reply.started":"2025-11-17T19:39:16.454485Z","shell.execute_reply":"2025-11-17T19:39:16.459263Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"dict_keys(['transformer_revin_True', 'transformer_revin_False'])"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"print(min(results['transformer']['losses']['train']['yield']),max(results['transformer']['losses']['train']['yield'][-100:]))\nfig, ax = plt.subplots(1,2, sharey=True)\nax[0].loglog(results['transformer']['losses']['train']['yield'])\nax[1].loglog(results['transformer']['losses']['val']['yield'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:21:17.184088Z","iopub.execute_input":"2025-11-17T19:21:17.184377Z","iopub.status.idle":"2025-11-17T19:21:17.198387Z","shell.execute_reply.started":"2025-11-17T19:21:17.184354Z","shell.execute_reply":"2025-11-17T19:21:17.197355Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/4090328880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yield'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yield'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yield'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yield'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'transformer'"],"ename":"KeyError","evalue":"'transformer'","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"model = results['transformer_revin_True']['model']\nprint(model)\nyield_losses = []\nval_loader = DataLoader(val_wheat, batch_size = 512, shuffle = False)\n\nwith torch.no_grad():\n    model.eval()\n    for inputs, targets in val_loader:\n    # Run data through model and backprop losses\n        predictions, _ = model(inputs)\n        pred_loss, yield_loss = model.compute_losses(predictions, inputs, targets)\n        yield_losses.append(yield_loss.item())\nprint(f'OOD loss: {np.mean(yield_losses)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:41:18.571146Z","iopub.execute_input":"2025-11-17T19:41:18.572038Z","iopub.status.idle":"2025-11-17T19:41:20.622321Z","shell.execute_reply.started":"2025-11-17T19:41:18.572013Z","shell.execute_reply":"2025-11-17T19:41:20.621548Z"}},"outputs":[{"name":"stdout","text":"FlexiblePredNet(\n  (input_projection): Linear(in_features=977, out_features=32, bias=True)\n  (sequence_model): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n        )\n        (linear1): Linear(in_features=32, out_features=16, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=16, out_features=32, bias=True)\n        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=32, out_features=1200, bias=True)\n  (mse): MSELoss()\n)\nOOD loss: 1.4866050533745268\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}